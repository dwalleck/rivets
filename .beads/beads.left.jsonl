{"id":"rivets-06w","content_hash":"5baa9903e0dcee2a1052c8215fb2ce156eafa76ad8050de0150c28b4e71897bd","title":"Implement core domain types (Issue, Dependency, Filter)","description":"Create the core domain types that represent issues, dependencies, filters, and related structures. These are used across all storage implementations and the CLI.","design":"Create domain module in rivets crate (src/domain/mod.rs):\n\n**Module Structure**:\n```\nsrc/domain/\n├── mod.rs          # Public exports\n├── issue.rs        # Issue, NewIssue, IssueUpdate\n├── dependency.rs   # Dependency, DependencyType\n├── filter.rs       # IssueFilter, builder\n└── types.rs        # IssueId, Priority, Status, IssueType\n```\n\n**Core Types**:\n```rust\n// Issue representation\npub struct Issue {\n    pub id: IssueId,\n    pub title: String,\n    pub description: String,\n    pub design: Option\u003cString\u003e,\n    pub acceptance_criteria: Option\u003cString\u003e,\n    pub notes: Option\u003cString\u003e,\n    pub status: Status,\n    pub priority: Priority,\n    pub issue_type: IssueType,\n    pub assignee: Option\u003cString\u003e,\n    pub created_at: DateTime\u003cUtc\u003e,\n    pub updated_at: DateTime\u003cUtc\u003e,\n    pub closed_at: Option\u003cDateTime\u003cUtc\u003e\u003e,\n    pub labels: Vec\u003cString\u003e,\n    pub dependencies: Vec\u003cDependency\u003e,\n}\n\npub struct NewIssue { /* builder fields */ }\npub struct IssueUpdate { /* optional fields */ }\n\n// Enums\npub enum Status { Open, InProgress, Blocked, Closed }\npub enum IssueType { Bug, Feature, Task, Epic, Chore }\npub enum DependencyType { Blocks, Related, ParentChild, DiscoveredFrom }\n\n// Filters\npub struct IssueFilter {\n    pub status: Option\u003cVec\u003cStatus\u003e\u003e,\n    pub priority: Option\u003cRangeInclusive\u003cu8\u003e\u003e,\n    pub assignee: Option\u003cString\u003e,\n    pub labels: Option\u003cVec\u003cString\u003e\u003e,\n    // ... all filter dimensions\n}\n\n// Newtypes for type safety\npub struct IssueId(String);\npub struct Priority(u8);  // 0-4\n```\n\n**Serde support**: All types derive Serialize/Deserialize for JSONL","acceptance_criteria":"- All core types defined\n- Serde derives for serialization\n- Builder patterns for NewIssue, IssueFilter\n- Validation logic (priority 0-4, ID format)\n- Type safety with newtypes\n- Unit tests for validation\n- Documentation with examples","notes":"Implement as src/domain/ module in rivets crate, NOT a separate rivets-core crate. This keeps the domain types co-located with the CLI application for now. Can be extracted to separate crate later if needed for library reuse.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-17T16:41:41.346359643-06:00","updated_at":"2025-11-17T19:25:08.707227678-06:00","closed_at":"2025-11-17T19:25:08.707227678-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-06w","depends_on_id":"rivets-0gc","type":"blocks","created_at":"2025-11-17T16:46:29.253520883-06:00","created_by":"daemon"}]}
{"id":"rivets-08u","content_hash":"047d53e190b5c637500a9ea4968226f9ade222b8f3b17c45bb26b7aa15f04143","title":"Implement map transformations for JsonlQuery","description":"Add map() method to JsonlQuery for transforming records during streaming.\n\nThis enables type transformations and projections on JSONL data.","design":"```rust\npub struct JsonlQuery\u003cT, U = T\u003e {\n    predicates: Vec\u003cBox\u003cdyn Fn(\u0026T) -\u003e bool + Send + Sync\u003e\u003e,\n    transform: Option\u003cBox\u003cdyn Fn(T) -\u003e U + Send + Sync\u003e\u003e,\n    _phantom: PhantomData\u003c(T, U)\u003e,\n}\n\nimpl\u003cT, U\u003e JsonlQuery\u003cT, U\u003e\nwhere\n    T: DeserializeOwned + 'static,\n    U: 'static,\n{\n    pub fn map\u003cV, F\u003e(self, transform: F) -\u003e JsonlQuery\u003cT, V\u003e\n    where\n        F: Fn(U) -\u003e V + Send + Sync + 'static,\n        V: 'static,\n    {\n        JsonlQuery {\n            predicates: self.predicates,\n            transform: Some(Box::new(move |t| transform(\n                self.transform.as_ref()\n                    .map(|f| f(t))\n                    .unwrap_or(t)\n            ))),\n            _phantom: PhantomData,\n        }\n    }\n}\n```\n\nNote: This requires refactoring JsonlQuery to support type transformations.","acceptance_criteria":"- map() method implemented\n- Supports type transformations\n- Can chain multiple map() calls\n- Transformations applied during streaming\n- Unit tests verify transformations\n- Integration test with filter + map pipeline","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-27T17:17:01.111641956-06:00","updated_at":"2025-11-27T17:17:01.111641956-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-08u","depends_on_id":"rivets-83j","type":"blocks","created_at":"2025-11-27T17:20:01.342007269-06:00","created_by":"dwalleck"}]}
{"id":"rivets-0gc","content_hash":"6bdd1fbf04700e68494941e86384a905165a248d71dab7ca0bb69a0283a6336e","title":"Design and implement storage trait abstraction","description":"Create the Storage trait abstraction that will allow multiple backend implementations (in-memory, JSONL, PostgreSQL). This is the foundation for the phased storage approach.\n\nPhase 1: Storage Trait + In-Memory Graph (MVP)\n- Define Storage trait with all required methods\n- In-memory implementation using HashMap + petgraph\n- JSONL persistence as backup/export format\n\nPhase 2: PostgreSQL with Recursive CTEs (Production)\n- PostgreSQL implementation with recursive CTEs for dependency queries\n- Migration path from in-memory to PostgreSQL\n\nThis task focuses on the trait definition and abstraction layer.\n\n## Clarifications\n\n### Session 2025-11-17\n\n- Q: Should the IssueStorage trait be async or synchronous? → A: Make trait async from start using async-trait crate (future-proof for PostgreSQL, enables non-blocking I/O)\n- Q: How should storage operations handle concurrent access in the in-memory backend? → A: Wrap in Arc\u003cMutex\u003cStorage\u003e\u003e for thread-safe access (standard pattern, simple for MVP)\n- Q: How should JSONL file corruption be handled during load? → A: Skip invalid lines with warning logs, continue loading valid entries (practical, allows recovery from partial corruption)\n- Q: Which tokio runtime should the CLI use? → A: Current-thread runtime (simpler for CLI, lower overhead, easier debugging)\n- Q: What should happen to dependencies when an issue is deleted? → A: Delete issue's dependencies, fail if issue has dependents with warning (safe, prevents orphaned references)","design":"## Storage Trait Design\n\n### Core Trait Definition\n\n**Async trait using async-trait crate:**\n\n```rust\nuse async_trait::async_trait;\n\n#[async_trait]\npub trait IssueStorage: Send + Sync {\n    // CRUD operations\n    async fn create(\u0026mut self, issue: NewIssue) -\u003e Result\u003cIssue\u003e;\n    async fn get(\u0026self, id: \u0026IssueId) -\u003e Result\u003cOption\u003cIssue\u003e\u003e;\n    async fn update(\u0026mut self, id: \u0026IssueId, updates: IssueUpdate) -\u003e Result\u003cIssue\u003e;\n    async fn delete(\u0026mut self, id: \u0026IssueId) -\u003e Result\u003c()\u003e;\n    \n    // Dependency management\n    async fn add_dependency(\u0026mut self, from: \u0026IssueId, to: \u0026IssueId, dep_type: DependencyType) -\u003e Result\u003c()\u003e;\n    async fn remove_dependency(\u0026mut self, from: \u0026IssueId, to: \u0026IssueId) -\u003e Result\u003c()\u003e;\n    async fn get_dependencies(\u0026self, id: \u0026IssueId) -\u003e Result\u003cVec\u003cDependency\u003e\u003e;\n    async fn get_dependents(\u0026self, id: \u0026IssueId) -\u003e Result\u003cVec\u003cDependency\u003e\u003e;\n    async fn has_cycle(\u0026self, from: \u0026IssueId, to: \u0026IssueId) -\u003e Result\u003cbool\u003e;\n    \n    // Queries\n    async fn list(\u0026self, filter: \u0026IssueFilter) -\u003e Result\u003cVec\u003cIssue\u003e\u003e;\n    async fn ready_to_work(\u0026self, filter: Option\u003c\u0026IssueFilter\u003e) -\u003e Result\u003cVec\u003cIssue\u003e\u003e;\n    async fn blocked_issues(\u0026self) -\u003e Result\u003cVec\u003c(Issue, Vec\u003cIssue\u003e)\u003e\u003e;\n    \n    // Batch operations for JSONL import/export\n    async fn import_issues(\u0026mut self, issues: Vec\u003cIssue\u003e) -\u003e Result\u003c()\u003e;\n    async fn export_all(\u0026self) -\u003e Result\u003cVec\u003cIssue\u003e\u003e;\n    \n    // Persistence (for backends that need explicit save)\n    async fn save(\u0026self) -\u003e Result\u003c()\u003e;\n}\n```\n\n**Note**: In-memory implementation will use blocking operations wrapped in `async` blocks for Phase 1. PostgreSQL will use true async I/O with sqlx in Phase 2. The save() method allows backends to persist state - InMemoryStorage saves to JSONL, PostgreSQL is a no-op.\n\n### Async Runtime Configuration\n\n**Use current-thread tokio runtime for CLI:**\n\n```rust\n// main.rs\n#[tokio::main(flavor = \"current_thread\")]\nasync fn main() -\u003e Result\u003c()\u003e {\n    // CLI initialization and execution\n    let cli = Cli::parse();\n    let config = Config::load().await?;\n    let storage = create_storage(config.storage_backend).await?;\n    \n    cli.execute(storage).await\n}\n```\n\n**Rationale**:\n- Simpler runtime for sequential CLI operations\n- Lower memory and CPU overhead\n- Easier to debug with single-threaded execution\n- Sufficient for I/O-bound storage operations\n- Can upgrade to multi-threaded if parallelism becomes necessary\n\n### Concurrency Model\n\n**Thread-safe wrapper for in-memory storage:**\n\n```rust\n// Storage implementation (not thread-safe)\nstruct InMemoryStorageInner {\n    issues: HashMap\u003cIssueId, Issue\u003e,\n    graph: DiGraph\u003cIssueId, DependencyType\u003e,\n    node_map: HashMap\u003cIssueId, NodeIndex\u003e,\n}\n\n// Thread-safe wrapper\npub type InMemoryStorage = Arc\u003cMutex\u003cInMemoryStorageInner\u003e\u003e;\n\n// CLI usage\npub struct App {\n    storage: Box\u003cdyn IssueStorage\u003e,  // Already thread-safe via Arc\u003cMutex\u003c\u003e\u003e\n    config: Config,\n}\n```\n\n**Rationale**: \n- Simple standard pattern for Rust CLI applications\n- Prevents data races with minimal complexity\n- Easy to test and reason about\n- PostgreSQL backend will handle concurrency at database level\n\n### Delete Semantics \u0026 Referential Integrity\n\n**Safe deletion with dependent check:**\n\n```rust\nasync fn delete(\u0026mut self, id: \u0026IssueId) -\u003e Result\u003c()\u003e {\n    // Check for dependents\n    let dependents = self.get_dependents(id).await?;\n    if !dependents.is_empty() {\n        let dependent_ids: Vec\u003c_\u003e = dependents.iter()\n            .map(|d| d.issue_id.to_string())\n            .collect();\n        return Err(Error::HasDependents {\n            issue_id: id.clone(),\n            dependents: dependent_ids,\n            message: format!(\n                \"Cannot delete {}: {} other issues depend on it. Delete or update those issues first.\",\n                id, dependents.len()\n            )\n        });\n    }\n    \n    // Remove all outgoing dependencies from this issue\n    let dependencies = self.get_dependencies(id).await?;\n    for dep in dependencies {\n        self.remove_dependency(id, \u0026dep.depends_on_id).await?;\n    }\n    \n    // Remove issue from storage\n    self.issues.remove(id);\n    if let Some(node_idx) = self.node_map.remove(id) {\n        self.graph.remove_node(node_idx);\n    }\n    \n    Ok(())\n}\n```\n\n**Strategy**: \n- Fail with clear error if issue has dependents\n- Error message lists dependent issue IDs\n- Remove all outgoing dependencies automatically\n- Prevents orphaned dependency references\n- User can use `--force` flag to batch delete dependents first (future enhancement)\n\n### Error Handling \u0026 Recovery\n\n**JSONL Corruption Handling:**\n\n```rust\npub async fn load_from_jsonl(path: \u0026Path) -\u003e Result\u003c(Self, Vec\u003cLoadError\u003e)\u003e {\n    let file = BufReader::new(File::open(path).await?);\n    let mut storage = Self::new();\n    let mut errors = Vec::new();\n    \n    for (line_num, line) in file.lines().enumerate() {\n        match line {\n            Ok(content) =\u003e {\n                match serde_json::from_str::\u003cIssue\u003e(\u0026content) {\n                    Ok(issue) =\u003e {\n                        if let Err(e) = storage.import_issue(issue).await {\n                            errors.push(LoadError::ImportFailed { line: line_num, reason: e });\n                            log::warn!(\"Line {}: Failed to import issue: {}\", line_num, e);\n                        }\n                    }\n                    Err(e) =\u003e {\n                        errors.push(LoadError::InvalidJson { line: line_num, reason: e });\n                        log::warn!(\"Line {}: Invalid JSON, skipping: {}\", line_num, e);\n                    }\n                }\n            }\n            Err(e) =\u003e {\n                errors.push(LoadError::ReadError { line: line_num, reason: e });\n                log::warn!(\"Line {}: Read error, skipping: {}\", line_num, e);\n            }\n        }\n    }\n    \n    if !errors.is_empty() {\n        log::warn!(\"Loaded with {} errors/warnings. {} issues imported.\", \n                   errors.len(), storage.issue_count());\n    }\n    \n    Ok((storage, errors))\n}\n```\n\n**Strategy**: Skip corrupted lines, log warnings, continue loading. Returns both the loaded storage and collection of errors for caller to handle.\n\n### Backend Factory\n\n```rust\npub enum StorageBackend {\n    InMemory,\n    Jsonl(PathBuf),\n    PostgreSQL(String), // connection string\n}\n\npub async fn create_storage(backend: StorageBackend) -\u003e Result\u003cBox\u003cdyn IssueStorage\u003e\u003e {\n    match backend {\n        StorageBackend::InMemory =\u003e Ok(Box::new(InMemoryStorage::new())),\n        StorageBackend::Jsonl(path) =\u003e {\n            let (storage, errors) = InMemoryStorage::load_from_jsonl(\u0026path).await?;\n            if !errors.is_empty() {\n                eprintln!(\"Warning: Loaded with {} errors. Check logs.\", errors.len());\n            }\n            Ok(Box::new(storage))\n        }\n        StorageBackend::PostgreSQL(conn_str) =\u003e {\n            Ok(Box::new(PostgresStorage::new(\u0026conn_str).await?))\n        }\n    }\n}\n```\n\n### Architecture Benefits\n\n1. **Testability**: Use in-memory storage for fast unit tests\n2. **Flexibility**: Easy to add new backends (SQLite, cloud storage, etc.)\n3. **Performance**: Can optimize each backend independently\n4. **Migration**: Clear path from simple to complex storage\n5. **Future-proof**: Async trait enables efficient PostgreSQL and network I/O\n6. **Thread-safety**: Arc\u003cMutex\u003c\u003e\u003e wrapper prevents data races\n7. **Resilience**: Graceful degradation on partial JSONL corruption\n8. **Simplicity**: Current-thread runtime reduces complexity for CLI use case\n9. **Data integrity**: Safe deletion prevents orphaned dependency references\n10. **Explicit persistence**: save() method allows control over when data is persisted","acceptance_criteria":"- Storage trait defined with async methods using async-trait in src/storage/mod.rs\n- Backend factory pattern implemented for creating storage instances (async)\n- Trait includes save() method for explicit persistence (async fn save(\u0026self) -\u003e Result\u003c()\u003e)\n- Trait documentation includes usage examples and backend comparison\n- All trait methods have clear error semantics\n- Trait is object-safe (can use Box\u003cdyn IssueStorage\u003e)\n- Unit tests for trait object usage with tokio test runtime\n- Documentation examples compile and run\n- async-trait dependency added to Cargo.toml\n- tokio dependency with current_thread flavor configured in main.rs\n- JSONL load handles corruption gracefully (skip invalid lines, log warnings)\n- Load function returns both storage and error collection\n- Delete operation checks for dependents, fails with clear error message if found\n- Delete operation removes all outgoing dependencies automatically\n- Error type includes HasDependents variant with issue list","status":"closed","priority":1,"issue_type":"task","assignee":"claude","created_at":"2025-11-17T16:15:21.281073361-06:00","updated_at":"2025-11-17T18:26:22.170944324-06:00","closed_at":"2025-11-17T18:26:22.170944324-06:00","source_repo":"."}
{"id":"rivets-2ub","content_hash":"85923e53d493088d406a3ba9ce07178985acee8c9e0dd1299c6d05bab6414376","title":"Implement Cargo workspace structure with directory layout","description":"Create the physical workspace structure including workspace Cargo.toml, crates/ directory, and initial Cargo.toml files for both rivets-jsonl and rivets crates. Set up the directory tree according to the design from rivets-kr3.","design":"Follow the design from rivets-kr3:\n- Create workspace root Cargo.toml with resolver=3 and workspace configuration\n- Create crates/ directory\n- Create crates/rivets-jsonl/ with Cargo.toml, src/, tests/, benches/, examples/ directories\n- Create crates/rivets/ with Cargo.toml, src/, tests/ directories\n- Create docs/ directory\n- Set up workspace dependencies in root Cargo.toml\n- Configure individual crate Cargo.toml files with workspace inheritance\n- Add README.md files for each crate","acceptance_criteria":"- Workspace Cargo.toml exists with correct resolver and members\n- crates/rivets-jsonl/Cargo.toml configured with workspace inheritance\n- crates/rivets/Cargo.toml configured with workspace inheritance and binary target\n- All required directories created\n- Project builds successfully with `cargo build`\n- README.md files created for workspace root and both crates","status":"closed","priority":1,"issue_type":"task","assignee":"Claude","created_at":"2025-11-17T15:45:32.798758607-06:00","updated_at":"2025-11-17T15:51:12.910655343-06:00","closed_at":"2025-11-17T15:51:12.910655343-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-2ub","depends_on_id":"rivets-kr3","type":"blocks","created_at":"2025-11-17T15:45:32.800932514-06:00","created_by":"daemon"}]}
{"id":"rivets-2wp","content_hash":"47d701fc257994a6b98d7a2cd6a4a3293b2d748a3275ce74c10579fa0452d789","title":"Create architecture document and diagrams for rivets","description":"Based on research from JSONL library design and project structure planning, create comprehensive architecture documentation with diagrams showing component relationships, data flow, and system design.","design":"Create documentation covering:\n- High-level system architecture diagram\n- Component/crate interaction diagrams\n- Data flow diagrams (how JSONL data moves through system)\n- Module organization within each crate\n- Key design decisions and rationale\n- Technology choices (libraries, patterns, etc.)\n- Future extensibility considerations","acceptance_criteria":"- Architecture document created (ADR or similar format)\n- System architecture diagram showing all components\n- Data flow diagram showing JSONL operations\n- Component interaction diagram\n- Document stored in repository (docs/architecture.md or similar)\n- Diagrams in a maintainable format (mermaid, plantuml, or similar)","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-17T15:08:45.698527855-06:00","updated_at":"2025-11-17T15:08:45.698527855-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-2wp","depends_on_id":"rivets-fk9","type":"blocks","created_at":"2025-11-17T15:08:54.237721515-06:00","created_by":"daemon"},{"issue_id":"rivets-2wp","depends_on_id":"rivets-kr3","type":"blocks","created_at":"2025-11-17T15:08:55.20642145-06:00","created_by":"daemon"}]}
{"id":"rivets-37y","content_hash":"876c7b051d0b74e53124881cd189062a199fb33ee25322e5499fb29ba811e902","title":"Implement labels and comments system","description":"Implement label tagging and commenting functionality for issues, with many-to-many relationships and filtering support.","design":"Based on beads labels.go and comments functionality:\n\n**Labels**:\n- Many-to-many relationship (issues ↔ labels)\n- Operations: add, remove, list labels on issue\n- Query by labels (AND / OR semantics)\n- Label management (create, delete, rename)\n\n**Comments**:\n- One-to-many (issue → comments)\n- Fields: author, created_at, content\n- Operations: add comment, list comments\n- Threaded comments (optional future)\n\n**Schema**:\n```sql\nlabels (\n  id INTEGER PRIMARY KEY,\n  issue_id TEXT NOT NULL,\n  label TEXT NOT NULL,\n  UNIQUE(issue_id, label)\n)\n\ncomments (\n  id INTEGER PRIMARY KEY,\n  issue_id TEXT NOT NULL,\n  author TEXT,\n  created_at TIMESTAMP,\n  content TEXT NOT NULL\n)\n```\n\n**CLI Commands**:\n- `rivets label add \u003cissue\u003e \u003clabel...\u003e`\n- `rivets label remove \u003cissue\u003e \u003clabel...\u003e`\n- `rivets comment add \u003cissue\u003e \u003ctext\u003e`\n- `rivets comment list \u003cissue\u003e`","acceptance_criteria":"- Label add/remove operations work\n- Comment add/list operations work\n- Query issues by labels (--label, --labels-any)\n- Labels embedded in JSONL export\n- Comments embedded in JSONL export\n- Foreign key CASCADE on issue deletion\n- Unit tests for label/comment CRUD","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-17T16:16:47.355222934-06:00","updated_at":"2025-11-17T16:16:47.355222934-06:00","source_repo":"."}
{"id":"rivets-3q4","content_hash":"6d50545e1a772082d43bd17822902f685141e83058f55d00196fa906ae41c294","title":"Add throughput benchmarks","description":"Create throughput benchmarks to verify \"100MB file in \u003c1s\" read and write targets.\n\nMeasures actual I/O performance with realistic data.","design":"Create benches/throughput_benchmarks.rs:\n\n```rust\nuse criterion::{criterion_group, criterion_main, Criterion, Throughput};\n\nfn throughput_benchmarks(c: \u0026mut Criterion) {\n    let mut group = c.benchmark_group(\"throughput\");\n    \n    // Read benchmarks\n    group.throughput(Throughput::Bytes(100 * 1024 * 1024)); // 100MB\n    group.bench_function(\"read_100mb\", |b| {\n        b.iter(|| {\n            // Stream read 100MB JSONL file\n        });\n    });\n    \n    // Write benchmarks\n    group.throughput(Throughput::Bytes(100 * 1024 * 1024));\n    group.bench_function(\"write_100mb\", |b| {\n        b.iter(|| {\n            // Write 100MB JSONL file\n        });\n    });\n    \n    // Atomic write benchmarks\n    group.throughput(Throughput::Bytes(100 * 1024 * 1024));\n    group.bench_function(\"write_100mb_atomic\", |b| {\n        b.iter(|| {\n            // Atomic write 100MB\n        });\n    });\n}\n```\n\nTargets from research:\n- Read: 100MB in \u003c1s (\u003e100MB/s)\n- Write: 100MB in \u003c1.5s (\u003e66MB/s)","acceptance_criteria":"- Throughput benchmarks created\n- Read 100MB benchmark passes \u003c1s target\n- Write 100MB benchmark passes \u003c1.5s target\n- Atomic write performance measured\n- Results documented in README\n- cargo bench runs successfully","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-27T17:17:52.51900859-06:00","updated_at":"2025-11-27T17:17:52.51900859-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-3q4","depends_on_id":"rivets-t0k","type":"blocks","created_at":"2025-11-27T17:20:24.32903356-06:00","created_by":"dwalleck"}]}
{"id":"rivets-3r7","content_hash":"955703375482cf59b73ee6129abe106c8920ff53ed63d113e27907fc7662de0a","title":"Add comprehensive unit tests for Phase 1","description":"Create comprehensive unit tests covering all Phase 1 functionality: reading, writing, streaming, and atomic writes.\n\nTests should cover happy paths, error cases, edge cases, and integration scenarios.","design":"Test categories:\n\n**Reader tests**:\n- read_line() with valid JSON\n- read_line() with malformed JSON\n- read_line() with empty lines\n- read_line() EOF handling\n- stream() with multiple records\n- stream() with errors mid-stream\n\n**Writer tests**:\n- write() single record\n- write_all() multiple records\n- flush() behavior\n- Serialization error handling\n\n**Integration tests**:\n- Round-trip: write then read\n- Atomic write success\n- Atomic write failure (temp file cleanup)\n- Large file streaming (memory usage)\n\nCreate tests/ directory with integration_tests.rs.","acceptance_criteria":"- Unit tests for all JsonlReader methods\n- Unit tests for all JsonlWriter methods\n- Integration tests for round-trip scenarios\n- Tests for error conditions\n- Tests for edge cases (empty files, large files)\n- All tests pass\n- Test coverage \u003e80% for Phase 1 code","status":"open","priority":1,"issue_type":"task","created_at":"2025-11-27T17:15:03.058434829-06:00","updated_at":"2025-11-27T17:15:03.058434829-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-3r7","depends_on_id":"rivets-zy0","type":"blocks","created_at":"2025-11-27T17:19:09.744817982-06:00","created_by":"dwalleck"},{"issue_id":"rivets-3r7","depends_on_id":"rivets-b1n","type":"blocks","created_at":"2025-11-27T17:19:15.528746481-06:00","created_by":"dwalleck"}]}
{"id":"rivets-4l2","content_hash":"0384e8f07ad8ab5617c43337914224ecaf5566448f0b07536d8b75748130fe0a","title":"Implement init command and workspace setup","description":"Implement the init command that creates the .rivets directory structure, initializes the SQLite database, and sets up configuration.","design":"Based on MVP architecture (in-memory + JSONL):\n\n**Directory Structure**:\n```\n.rivets/\n├── issues.jsonl        # JSONL data file\n├── config.yaml         # Project config\n└── .gitignore          # Ignore metadata files\n```\n\n**Init Process**:\n1. Check if already initialized (error if exists)\n2. Create .rivets/ directory\n3. Create default config.yaml with storage backend settings\n4. Set issue prefix (from flag or prompt)\n5. Create .gitignore\n6. Create empty issues.jsonl file\n\n**Git Integration**:\n- Add .rivets to .gitignore root if not present\n- Suggest git commit for initial setup\n\n**CLI**:\n```bash\nrivets init [--prefix \u003cprefix\u003e] [--quiet]\n```\n\n**Config Template**:\n```yaml\nissue-prefix: \"proj\"\nstorage:\n  backend: \"memory\"\n  data_file: \".rivets/issues.jsonl\"\n```","acceptance_criteria":"- Init creates .rivets directory\n- config.yaml created with storage backend = memory\n- issues.jsonl created (empty initially)\n- .gitignore created\n- Prefix validation (alphanumeric, 2-20 chars)\n- Error if already initialized\n- Integration test verifies complete setup\n- No database files created","status":"open","priority":1,"issue_type":"task","created_at":"2025-11-17T16:16:47.684395549-06:00","updated_at":"2025-11-17T17:02:47.197196221-06:00","source_repo":"."}
{"id":"rivets-4q2","content_hash":"d374e126dccef26fecb8959c849a329973ffd370943c2e9d5b0569dec546559a","title":"Integrate resilient JSONL loading with in_memory storage","description":"Update rivets in_memory::load_from_jsonl() to use the new rivets-jsonl library's resilient loading functionality.\n\nThis replaces the current manual JSONL parsing with the library implementation.","design":"Update in_memory.rs to use rivets-jsonl:\n\n```rust\nuse rivets_jsonl::read_jsonl_resilient;\n\npub async fn load_from_jsonl(\n    path: \u0026Path,\n    prefix: String,\n) -\u003e Result\u003c(Box\u003cdyn IssueStorage\u003e, Vec\u003cLoadWarning\u003e)\u003e {\n    let (issues, warnings) = read_jsonl_resilient::\u003cIssue, _\u003e(path).await?;\n    \n    // Convert rivets_jsonl::Warning to LoadWarning\n    let load_warnings: Vec\u003cLoadWarning\u003e = warnings\n        .into_iter()\n        .map(|w| match w {\n            rivets_jsonl::Warning::MalformedJson { line_number, error } =\u003e {\n                LoadWarning::MalformedJson { line_number, error }\n            }\n            rivets_jsonl::Warning::SkippedLine { line_number, reason } =\u003e {\n                // Map to appropriate LoadWarning variant\n                LoadWarning::MalformedJson { line_number, error: reason }\n            }\n        })\n        .collect();\n    \n    // Continue with existing dependency reconstruction logic\n    let storage = Arc::new(Mutex::new(InMemoryStorageInner::new(prefix)));\n    // ... existing code\n    \n    Ok((Box::new(storage), load_warnings))\n}\n```\n\nAdd rivets-jsonl as dependency in rivets/Cargo.toml.","acceptance_criteria":"- in_memory::load_from_jsonl uses rivets-jsonl library\n- Warnings correctly mapped between types\n- All existing tests still pass\n- No performance regression\n- Dependency added to Cargo.toml","status":"open","priority":1,"issue_type":"task","created_at":"2025-11-27T17:16:06.671886113-06:00","updated_at":"2025-11-27T17:16:06.671886113-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-4q2","depends_on_id":"rivets-uyg","type":"blocks","created_at":"2025-11-27T17:19:38.432794292-06:00","created_by":"dwalleck"}]}
{"id":"rivets-6op","content_hash":"d721a87b5f827080b07034c4e359a776efc4306f06ed28ae09afccd8de8f01d9","title":"Implement dependency system with cycle detection","description":"Implement the 4-type dependency system (blocks, related, parent-child, discovered-from) with recursive CTE-based cycle detection to prevent circular dependencies.","design":"Based on beads dependencies.go, adapted for Phase 1 (in-memory + petgraph):\n\n**Dependency Types**:\n1. blocks: Hard blocker (prevents work)\n2. related: Soft link (informational)\n3. parent-child: Hierarchical (epics→tasks)\n4. discovered-from: Found during work\n\n**Phase 1 Implementation (In-Memory + petgraph)**:\n```rust\nimpl InMemoryStorage {\n    fn add_dependency(\n        \u0026mut self, \n        from: \u0026IssueId, \n        to: \u0026IssueId, \n        dep_type: DependencyType\n    ) -\u003e Result\u003c()\u003e {\n        // Cycle detection using petgraph\n        if self.has_cycle(from, to)? {\n            return Err(Error::CircularDependency);\n        }\n        \n        // Add edge to graph\n        let from_node = self.node_map.get(from)?;\n        let to_node = self.node_map.get(to)?;\n        self.graph.add_edge(*from_node, *to_node, dep_type);\n        \n        // Update issue's dependency list\n        let issue = self.issues.get_mut(from)?;\n        issue.dependencies.push(Dependency { \n            depends_on_id: to.clone(), \n            dep_type \n        });\n        \n        Ok(())\n    }\n    \n    fn has_cycle(\u0026self, from: \u0026IssueId, to: \u0026IssueId) -\u003e Result\u003cbool\u003e {\n        use petgraph::algo;\n        let from_node = self.node_map.get(from)?;\n        let to_node = self.node_map.get(to)?;\n        \n        // Check if adding edge creates cycle\n        Ok(algo::has_path_connecting(\u0026self.graph, *to_node, *from_node, None))\n    }\n}\n```\n\n**Phase 3 (PostgreSQL)**: Will use recursive CTEs for cycle detection:\n```sql\nWITH RECURSIVE paths AS (\n    SELECT issue_id, depends_on_id, 1 as depth\n    FROM dependencies WHERE issue_id = ?\n    UNION ALL\n    SELECT d.issue_id, d.depends_on_id, p.depth + 1\n    FROM dependencies d JOIN paths p \n    ON d.issue_id = p.depends_on_id\n    WHERE p.depth \u003c 100\n)\nSELECT EXISTS(SELECT 1 FROM paths WHERE depends_on_id = ?)\n```","acceptance_criteria":"- All 4 dependency types supported\n- Add dependency with cycle pre-check using petgraph\n- Remove dependency operation\n- List dependencies (forward/reverse) \n- Cycle detection via petgraph has_path_connecting\n- Find dependency tree via graph traversal\n- Unit tests for cycles, trees, all dep types\n- Benchmark: cycle check for 1000 issues in \u003c10ms","status":"open","priority":1,"issue_type":"task","created_at":"2025-11-17T16:15:21.434804051-06:00","updated_at":"2025-11-17T17:03:27.144130158-06:00","source_repo":"."}
{"id":"rivets-6p4","content_hash":"9041f1459d0b32c729951e84a63676c6b05cb4127f77092afc4b391003df3209","title":"Implement query execution with filtering","description":"Implement the execute() method for JsonlQuery that applies filters during streaming.\n\nThis enables filtering JSONL records without loading entire file into memory.","design":"```rust\nimpl\u003cT\u003e JsonlQuery\u003cT\u003e\nwhere\n    T: DeserializeOwned + 'static,\n{\n    pub fn execute\u003cR\u003e(\n        self,\n        reader: R\n    ) -\u003e impl Stream\u003cItem = Result\u003cT\u003e\u003e\n    where\n        R: AsyncRead + Unpin,\n    {\n        let jsonl_reader = JsonlReader::new(reader);\n        let stream = jsonl_reader.stream::\u003cT\u003e();\n        \n        stream.filter_map(move |result| {\n            let matches = match \u0026result {\n                Ok(value) =\u003e self.matches(value),\n                Err(_) =\u003e true, // Pass errors through\n            };\n            \n            async move {\n                if matches {\n                    Some(result)\n                } else {\n                    None\n                }\n            }\n        })\n    }\n}\n```","acceptance_criteria":"- execute() method implemented\n- Returns Stream\u003cItem = Result\u003cT\u003e\u003e\n- Filters applied during streaming\n- Only matching records yielded\n- Errors passed through unchanged\n- Memory usage constant (streaming)\n- Unit tests verify filtering behavior\n- Integration test with large file","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-27T17:16:55.357983235-06:00","updated_at":"2025-11-27T17:16:55.357983235-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-6p4","depends_on_id":"rivets-83j","type":"blocks","created_at":"2025-11-27T17:19:55.646172247-06:00","created_by":"dwalleck"}]}
{"id":"rivets-6pi","content_hash":"d77bd80b53f88e6742b0ffd3213fc732141d4066c4877abfcc9a23449d46fd0e","title":"Implement JsonlWriter::write() and write_all() methods","description":"Implement write methods for JsonlWriter that serialize and write records to the output stream.\n\nIncludes both single-record write() and batch write_all() for efficiency.","design":"```rust\nuse serde::Serialize;\nuse tokio::io::AsyncWriteExt;\n\nimpl\u003cW: AsyncWrite + Unpin\u003e JsonlWriter\u003cW\u003e {\n    pub async fn write\u003cT: Serialize\u003e(\u0026mut self, value: \u0026T) -\u003e Result\u003c()\u003e {\n        let json = serde_json::to_string(value)?;\n        self.writer.write_all(json.as_bytes()).await?;\n        self.writer.write_all(b\"\\n\").await?;\n        Ok(())\n    }\n    \n    pub async fn write_all\u003cT: Serialize\u003e(\n        \u0026mut self,\n        values: impl IntoIterator\u003cItem = T\u003e,\n    ) -\u003e Result\u003c()\u003e {\n        for value in values {\n            self.write(\u0026value).await?;\n        }\n        Ok(())\n    }\n    \n    pub async fn flush(\u0026mut self) -\u003e Result\u003c()\u003e {\n        self.writer.flush().await?;\n        Ok(())\n    }\n}\n```","acceptance_criteria":"- write() method serializes and writes single record\n- write_all() writes multiple records efficiently\n- flush() method flushes buffered data\n- Each record terminated with newline\n- Proper error handling for serialization failures\n- Unit tests verify writing single/multiple records\n- Integration test with read/write round-trip","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T17:14:51.609289345-06:00","updated_at":"2025-11-27T18:21:22.277922663-06:00","closed_at":"2025-11-27T18:21:22.277922663-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-6pi","depends_on_id":"rivets-uo7","type":"blocks","created_at":"2025-11-27T17:18:58.168325594-06:00","created_by":"dwalleck"}]}
{"id":"rivets-6tl","content_hash":"abfe3b87cb3c18b9c3d5544936c74218d0861206e5f293104f53a9c8e0956473","title":"Implement issue filtering and query system","description":"Implement the comprehensive filtering system for querying issues by status, priority, type, assignee, labels, dates, and text search.","design":"Based on beads IssueFilter struct and query builder:\n\n**Filter Dimensions**:\n- **Status**: open, in_progress, blocked, closed\n- **Priority**: 0-4, ranges (P0-P2), min/max\n- **Type**: bug, feature, task, epic, chore\n- **Assignee**: specific user, empty\n- **Labels**: AND (all required), OR (any match)\n- **Dates**: created_after/before, updated_after/before, closed_after/before\n- **Text**: title_contains, description_contains, notes_contains\n- **Flags**: empty_description, no_assignee, no_labels\n- **IDs**: specific list\n\n**Query Builder Pattern**:\n```rust\nIssueFilter::builder()\n    .status(vec![Status::Open, Status::InProgress])\n    .priority_range(0..=2)\n    .labels_all(vec![\"bug\", \"urgent\"])\n    .created_after(date)\n    .limit(100)\n    .build()\n```\n\n**SQL Generation**:\n- Dynamic WHERE clause construction\n- Parameterized queries (SQL injection safe)\n- Index-friendly query plans","acceptance_criteria":"- All filter dimensions functional\n- Complex multi-filter queries work\n- Query builder API type-safe\n- SQL generation correct and efficient\n- Performance acceptable (1000 issues in \u003c50ms)\n- Unit tests for edge cases\n- Integration tests for complex filters","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-17T16:16:47.522142136-06:00","updated_at":"2025-11-17T16:16:47.522142136-06:00","source_repo":"."}
{"id":"rivets-7q6","content_hash":"30aee685cbc27c7411fb279d3743bc4a499aaa9f6efc00f774c7c6bc363547cf","title":"Add Send assertions for async futures (M-TYPES-SEND)","description":"The codebase uses async/await extensively but lacks explicit compile-time assertions that futures are Send, violating the M-TYPES-SEND guideline.\n\nCurrent State:\n- IssueStorage trait requires Send + Sync\n- Methods return futures implicitly\n- No compile-time verification that futures are actually Send\n- Could break multi-threaded async runtimes\n\nRisk:\n- If a future accidentally captures non-Send types, it won't be detected until runtime\n- Makes the library unsafe to use with work-stealing executors","design":"Add Send assertions in storage/mod.rs tests (after line 453):\n\n```rust\n#[test]\nfn assert_storage_futures_are_send() {\n    const fn assert_send\u003cT: Send\u003e() {}\n    \n    // Assert that all IssueStorage methods return Send futures\n    assert_send::\u003cBox\u003cdyn IssueStorage\u003e\u003e();\n    \n    // Can also test specific future types if needed\n    // This ensures futures work with multi-threaded runtimes\n}\n```\n\nThis pattern:\n- Runs at compile time (const fn)\n- Catches non-Send futures immediately\n- Documents the Send requirement\n- Zero runtime cost","acceptance_criteria":"- [ ] assert_send helper function added\n- [ ] Test verifies Box\u003cdyn IssueStorage\u003e is Send\n- [ ] Test verifies key future types are Send\n- [ ] Code compiles (proving assertion holds)\n- [ ] All tests pass","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-27T16:49:09.918623993-06:00","updated_at":"2025-11-27T16:49:09.918623993-06:00","source_repo":".","labels":["async","code-quality","testing"]}
{"id":"rivets-7qb","content_hash":"2771f3a324beb0e5d5fbb39b7216385985c5c1ac425fe215bf5441fb55baf0dc","title":"Memory profiling and optimization","description":"Profile memory usage of rivets-jsonl and optimize to meet the \u003c10MB target regardless of file size.\n\nUses memory profiling tools and techniques to verify streaming guarantees.","design":"Memory profiling approach:\n\n1. **Create test files**:\n   - 1MB (1K records)\n   - 10MB (10K records)\n   - 100MB (100K records)\n   - 1GB (1M records)\n\n2. **Profile streaming operations**:\n   ```rust\n   #[cfg(test)]\n   mod memory_tests {\n       use std::alloc::{GlobalAlloc, Layout, System};\n       use std::sync::atomic::{AtomicUsize, Ordering};\n       \n       struct TrackingAllocator;\n       \n       static ALLOCATED: AtomicUsize = AtomicUsize::new(0);\n       \n       unsafe impl GlobalAlloc for TrackingAllocator {\n           unsafe fn alloc(\u0026self, layout: Layout) -\u003e *mut u8 {\n               ALLOCATED.fetch_add(layout.size(), Ordering::SeqCst);\n               System.alloc(layout)\n           }\n           \n           unsafe fn dealloc(\u0026self, ptr: *mut u8, layout: Layout) {\n               ALLOCATED.fetch_sub(layout.size(), Ordering::SeqCst);\n               System.dealloc(ptr, layout);\n           }\n       }\n   }\n   ```\n\n3. **Optimization targets**:\n   - Buffer size tuning\n   - Reduce allocations in hot paths\n   - Reuse String buffers where possible\n\n4. **Verification**:\n   - Peak memory usage \u003c10MB for all file sizes\n   - Document peak memory in README","acceptance_criteria":"- Memory profiling tests created\n- Tested with 1MB, 10MB, 100MB, 1GB files\n- Peak memory \u003c10MB for all sizes\n- Buffer sizes optimized\n- Allocations minimized in hot paths\n- Results documented","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-27T17:17:46.767467084-06:00","updated_at":"2025-11-27T17:17:46.767467084-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-7qb","depends_on_id":"rivets-t0k","type":"blocks","created_at":"2025-11-27T17:20:18.598804542-06:00","created_by":"dwalleck"}]}
{"id":"rivets-83j","content_hash":"4552c2e9d372becf69a84f1ff3b1b01984e0b01f29a2fc3e7bfce10f5c162ab4","title":"Implement JsonlQuery builder pattern","description":"Implement the JsonlQuery struct with builder pattern for constructing query pipelines with filters and transformations.\n\nThis provides a fluent API for filtering JSONL streams during reading.","design":"```rust\nuse std::marker::PhantomData;\n\npub struct JsonlQuery\u003cT\u003e {\n    predicates: Vec\u003cBox\u003cdyn Fn(\u0026T) -\u003e bool + Send + Sync\u003e\u003e,\n    _phantom: PhantomData\u003cT\u003e,\n}\n\nimpl\u003cT\u003e JsonlQuery\u003cT\u003e\nwhere\n    T: DeserializeOwned + 'static,\n{\n    pub fn new() -\u003e Self {\n        Self {\n            predicates: Vec::new(),\n            _phantom: PhantomData,\n        }\n    }\n    \n    pub fn filter\u003cF\u003e(mut self, predicate: F) -\u003e Self\n    where\n        F: Fn(\u0026T) -\u003e bool + Send + Sync + 'static,\n    {\n        self.predicates.push(Box::new(predicate));\n        self\n    }\n    \n    fn matches(\u0026self, value: \u0026T) -\u003e bool {\n        self.predicates.iter().all(|pred| pred(value))\n    }\n}\n```","acceptance_criteria":"- JsonlQuery struct defined\n- new() constructor\n- filter() method adds predicates\n- Predicates stored as trait objects\n- Builder pattern allows chaining\n- Unit tests verify builder functionality\n- Compiles without errors","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-27T17:16:49.651063411-06:00","updated_at":"2025-11-27T17:16:49.651063411-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-83j","depends_on_id":"rivets-zy0","type":"blocks","created_at":"2025-11-27T17:19:49.936527243-06:00","created_by":"dwalleck"}]}
{"id":"rivets-8yl","content_hash":"230be8cd7dbffca52079a3c6d2b98ee32ff3e76aaece72c8b6eab14dddd19e69","title":"Implement resilient streaming for JsonlReader","description":"Implement stream_resilient() method that continues reading despite malformed JSON lines, collecting warnings instead of returning errors.\n\nThis provides the same resilient loading behavior as in_memory::load_from_jsonl().","design":"```rust\nimpl\u003cR: AsyncRead + Unpin\u003e JsonlReader\u003cR\u003e {\n    pub fn stream_resilient\u003cT\u003e(\n        self\n    ) -\u003e (impl Stream\u003cItem = T\u003e, WarningCollector)\n    where\n        T: DeserializeOwned + 'static,\n    {\n        let collector = WarningCollector::new();\n        let collector_clone = collector.clone();\n        \n        let stream = futures::stream::unfold(\n            (self, collector_clone),\n            |(mut reader, collector)| async move {\n                loop {\n                    match reader.read_line().await {\n                        Ok(Some(value)) =\u003e return Some((value, (reader, collector))),\n                        Ok(None) =\u003e return None, // EOF\n                        Err(e) =\u003e {\n                            // Collect warning and continue\n                            collector.add(Warning::MalformedJson {\n                                line_number: reader.line_number,\n                                error: e.to_string(),\n                            });\n                            // Continue to next line\n                            continue;\n                        }\n                    }\n                }\n            },\n        );\n        \n        (stream, collector)\n    }\n}\n```","acceptance_criteria":"- stream_resilient() method implemented\n- Returns (Stream\u003cItem = T\u003e, WarningCollector)\n- Continues reading on malformed JSON\n- Warnings collected for each error\n- Stream yields only successfully parsed records\n- Unit tests verify resilient behavior\n- Integration test with mixed valid/invalid JSONL","status":"open","priority":1,"issue_type":"task","created_at":"2025-11-27T17:15:55.303102489-06:00","updated_at":"2025-11-27T17:15:55.303102489-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-8yl","depends_on_id":"rivets-dgt","type":"blocks","created_at":"2025-11-27T17:19:27.008485366-06:00","created_by":"dwalleck"}]}
{"id":"rivets-95l","content_hash":"5dbd8e98e9df95793addaaa023fbf234efc925fe1d1da759d7f6b55f0f3cc9c6","title":"Add local quality gates using cargo-husky","description":"Add cargo husky and formatting, linting, and testing as pre-commit gates","design":"Implemented pre-commit hook combining beads sync and cargo quality gates:\n\n**Hook Location**: `.git/hooks/pre-commit`\n\n**Quality Checks**:\n1. Beads sync flush (if in beads workspace)\n2. `cargo fmt -- --check` - Code formatting\n3. `cargo clippy --all-targets --all-features -- -D warnings` - Linting\n4. `cargo test --quiet` - All tests\n\n**Stub Code Handling**: Added `#[allow(dead_code)]` attributes to placeholder types/functions to prevent false positives during development.\n\n**Manual Execution**: Hook can be tested by running `.git/hooks/pre-commit` directly.","acceptance_criteria":"✓ Pre-commit hook installed at .git/hooks/pre-commit\n✓ Runs cargo fmt check before commits\n✓ Runs cargo clippy with -D warnings before commits\n✓ Runs cargo test before commits\n✓ Integrates with existing beads sync hook\n✓ Documentation added to README.md\n✓ Successfully passes all checks on current codebase","status":"closed","priority":2,"issue_type":"task","assignee":"Claude","created_at":"2025-11-17T15:55:30.160010922-06:00","updated_at":"2025-11-17T16:04:29.413337487-06:00","closed_at":"2025-11-17T16:04:29.413337487-06:00","source_repo":"."}
{"id":"rivets-9mh","content_hash":"23eb8d64cc0ef7eeb517fc395f23b24a75570491d8a5e8615295c6d0a6ac3e33","title":"Implement daemon with auto-sync and RPC server","description":"Implement the background daemon process that provides auto-export (dirty issues to JSONL), auto-import (JSONL to SQLite), and serves RPC requests.","design":"Based on beads internal/daemon/:\n\n**Daemon Responsibilities**:\n1. Auto-export with debouncing (5s default)\n2. Auto-import on JSONL mtime changes\n3. RPC server for CLI commands\n4. PID file management\n5. Graceful shutdown on signals\n\n**Lifecycle**:\n- Auto-start on first CLI command (if not running)\n- Background detached process\n- Socket cleanup on exit\n- Lock file prevents multiple daemons\n\n**Rust Stack**:\n- `tokio` runtime for async event loop\n- `notify` crate for file watching (optional)\n- `signal-hook` for signal handling\n- `daemonize` crate for backgrounding\n\n**Features**:\n- Debounced exports (collect changes, flush after timeout)\n- File watcher for imports\n- Health monitoring\n- Log rotation","acceptance_criteria":"- Daemon starts in background\n- Auto-export triggers after changes\n- Auto-import on JSONL modification\n- RPC server responds to requests\n- PID file created and managed\n- Graceful shutdown on SIGTERM\n- Multiple daemon prevention\n- Integration tests verify auto-sync behavior","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-17T16:16:07.330511078-06:00","updated_at":"2025-11-17T16:16:07.330511078-06:00","source_repo":"."}
{"id":"rivets-azn","content_hash":"08d6b95be302f095481d6773f7e9548e2d26d02d9ab7c4c81bc92906a809cd7b","title":"Document rivets architecture and API design","description":"Create comprehensive architecture documentation for rivets including crate structure, module organization, data flow diagrams, and public API documentation.","design":"Create docs/architecture.md covering:\n\n**1. System Architecture**:\n- Crate dependency graph\n- Component interaction diagram\n- Data flow (CLI → RPC → Storage → JSONL)\n\n**2. Storage Layer**:\n- Database schema diagram\n- Table relationships\n- Index strategy\n- Migration system\n\n**3. ID Generation**:\n- Hash algorithm explanation\n- Collision handling\n- Hierarchical ID format\n\n**4. Dependency System**:\n- 4 dependency types\n- Cycle detection algorithm\n- Ready work calculation\n\n**5. JSONL Sync**:\n- Export flow diagram\n- Import flow diagram\n- Conflict resolution\n\n**6. RPC Protocol**:\n- Message format\n- Operation list\n- Error handling\n\nUse mermaid diagrams for visual clarity","acceptance_criteria":"- architecture.md created in docs/\n- All major components documented\n- Mermaid diagrams included\n- Data flow clearly explained\n- API surfaces documented\n- Design decisions justified\n- Future extensibility noted","status":"open","priority":3,"issue_type":"task","created_at":"2025-11-17T16:16:47.84827462-06:00","updated_at":"2025-11-17T16:16:47.84827462-06:00","source_repo":"."}
{"id":"rivets-b1n","content_hash":"e4a3a139e07fc185d2ecf68298dd560f06b1caa0a119e01aac97f80e588d245e","title":"Implement atomic write functionality","description":"Implement write_jsonl_atomic() convenience function that writes to a temp file then atomically renames it. This provides crash safety for JSONL persistence.\n\nUses the temp file + rename pattern for atomic writes on POSIX systems.","design":"```rust\nuse std::path::Path;\nuse tokio::fs::File;\n\npub async fn write_jsonl_atomic\u003cT, P\u003e(path: P, values: \u0026[T]) -\u003e Result\u003c()\u003e\nwhere\n    T: Serialize,\n    P: AsRef\u003cPath\u003e,\n{\n    let path = path.as_ref();\n    let temp_path = path.with_extension(\"tmp\");\n    \n    // Write to temp file\n    {\n        let file = File::create(\u0026temp_path).await?;\n        let mut writer = JsonlWriter::new(file);\n        writer.write_all(values.iter()).await?;\n        writer.flush().await?;\n    }\n    \n    // Atomic rename\n    tokio::fs::rename(\u0026temp_path, path).await?;\n    \n    Ok(())\n}\n```","acceptance_criteria":"- write_jsonl_atomic() function implemented\n- Writes to temp file with .tmp extension\n- Atomically renames temp file to target path\n- Temp file cleaned up on success\n- Proper error handling if write fails\n- Integration test verifies atomicity\n- Test verifies crash during write leaves original file intact","status":"open","priority":1,"issue_type":"task","created_at":"2025-11-27T17:14:57.33389314-06:00","updated_at":"2025-11-27T17:14:57.33389314-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-b1n","depends_on_id":"rivets-6pi","type":"blocks","created_at":"2025-11-27T17:19:03.964270123-06:00","created_by":"dwalleck"}]}
{"id":"rivets-bi2","content_hash":"22f5b811d2d5acce3b7408c7f97ab19e516b08a28c04f3bf1617a76f4ddae53f","title":"Implement RPC system for daemon communication","description":"Implement the RPC server/client system using Unix domain sockets (or named pipes on Windows) for CLI-daemon communication with JSON-RPC protocol.","design":"Based on beads internal/rpc/:\n\n**Architecture**:\n- Server runs in daemon process\n- Client in each CLI invocation\n- Transport: Unix sockets (Linux/macOS), Named pipes (Windows)\n- Protocol: JSON-RPC over newline-delimited messages\n\n**Operations Exposed**:\n- Full CRUD (create, read, update, delete)\n- Queries (list, ready, blocked, stats)\n- Import/export triggers\n- Health checks\n\n**Rust Stack**:\n- `tokio` for async I/O\n- `serde_json` for JSON-RPC\n- `interprocess` crate for IPC\n- Custom error types for RPC failures\n\n**Features**:\n- Version compatibility checks\n- Connection timeouts\n- Graceful shutdown\n- Request/response correlation","acceptance_criteria":"- RPC server accepts connections\n- RPC client can call server methods\n- JSON-RPC protocol implemented\n- All CRUD operations exposed\n- Health check endpoint works\n- Connection errors handled gracefully\n- Concurrent requests supported\n- Integration tests for RPC round-trips","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-17T16:16:07.159548647-06:00","updated_at":"2025-11-17T16:16:07.159548647-06:00","source_repo":"."}
{"id":"rivets-bsp","content_hash":"c483d43d0acf23835b41a8107f5c9e83770ac34de2d4e25fa1b3560888c01f84","title":"Implement core CLI commands (create, list, show, update, close, delete)","description":"Implement the essential CLI commands for basic issue management using clap for argument parsing. These are the most frequently used commands.","design":"Based on beads cmd/bd/:\n\n**Commands**:\n1. **create**: Create new issues (interactive or flags)\n2. **list**: Query issues with filters (status, priority, assignee, labels, etc.)\n3. **show**: Display full issue details with dependencies\n4. **update**: Modify issue fields\n5. **close**: Mark issues as closed with reason\n6. **delete**: Remove issues (with cascade confirmation)\n\n**Clap Structure**:\n```rust\n#[derive(Parser)]\nenum Commands {\n    Create(CreateArgs),\n    List(ListArgs),\n    Show(ShowArgs),\n    Update(UpdateArgs),\n    Close(CloseArgs),\n    Delete(DeleteArgs),\n}\n```\n\n**Features**:\n- Interactive prompts for missing data\n- JSON output mode (--json flag)\n- Batch operations where applicable\n- Validation and error messages","acceptance_criteria":"- All 6 commands implemented and working\n- --help shows usage for each command\n- Filters work correctly (status, priority, etc.)\n- JSON output mode functional\n- Interactive mode for create command\n- Error messages are clear and helpful\n- Integration tests verify command behavior","status":"open","priority":1,"issue_type":"task","created_at":"2025-11-17T16:16:06.835605991-06:00","updated_at":"2025-11-17T16:16:06.835605991-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-bsp","depends_on_id":"rivets-cgl","type":"blocks","created_at":"2025-11-17T17:02:47.758985777-06:00","created_by":"daemon"}]}
{"id":"rivets-bz5","content_hash":"7e3855ad9c10eef8a05a609878fe3a1c94ee014b624ed079260bfac97f0f7958","title":"Implement InMemoryStorage with petgraph for dependency graph","description":"Create the InMemoryStorage implementation using HashMap for issues and petgraph for the dependency graph. This is the first storage backend and enables fast MVP development.","design":"Based on storage trait from rivets-0gc, implement in-memory backend:\n\n**Structure (following rivets-0gc architecture):**\n\n```rust\n// Private inner storage (not thread-safe)\nstruct InMemoryStorageInner {\n    issues: HashMap\u003cIssueId, Issue\u003e,\n    graph: DiGraph\u003cIssueId, DependencyType\u003e,  // petgraph\n    node_map: HashMap\u003cIssueId, NodeIndex\u003e,\n}\n\n// Public thread-safe type alias\npub type InMemoryStorage = Arc\u003cMutex\u003cInMemoryStorageInner\u003e\u003e;\n```\n\n**Async Trait Implementation (blocking operations in async methods):**\n\n```rust\n#[async_trait]\nimpl IssueStorage for InMemoryStorage {\n    async fn create(\u0026mut self, new: NewIssue) -\u003e Result\u003cIssue\u003e {\n        let mut inner = self.lock().await;\n        let id = generate_hash_id(\u0026new);  // Uses rivets-x1e\n        let issue = Issue::from(new, id.clone());\n        inner.issues.insert(id.clone(), issue.clone());\n        let node = inner.graph.add_node(id.clone());\n        inner.node_map.insert(id.clone(), node);\n        Ok(issue)\n    }\n    \n    async fn has_cycle(\u0026self, from: \u0026IssueId, to: \u0026IssueId) -\u003e Result\u003cbool\u003e {\n        let inner = self.lock().await;\n        // Use petgraph's has_path_connecting\n        let from_node = inner.node_map.get(from)\n            .ok_or_else(|| Error::IssueNotFound(from.clone()))?;\n        let to_node = inner.node_map.get(to)\n            .ok_or_else(|| Error::IssueNotFound(to.clone()))?;\n        Ok(algo::has_path_connecting(\u0026inner.graph, *to_node, *from_node, None))\n    }\n    \n    async fn ready_to_work(\u0026self, filter: Option\u003c\u0026IssueFilter\u003e) -\u003e Result\u003cVec\u003cIssue\u003e\u003e {\n        let inner = self.lock().await;\n        // Graph traversal to find issues with no blocking dependencies\n        // Filter by status=open or in_progress\n        // Check all incoming edges for blocking dependencies\n        // Return issues with no blockers\n        // ... implementation\n    }\n}\n```\n\n**Key Implementation Notes:**\n- Use `Arc::new(Mutex::new(InMemoryStorageInner { ... }))` for initialization\n- Lock mutex for all operations: `let inner = self.lock().await;`\n- All petgraph and HashMap operations are blocking (acceptable for MVP)\n- Graph operations use petgraph::algo for cycle detection and traversal\n- Maintain node_map synchronization when adding/removing issues\n\n**Dependencies**: `petgraph = \\\"0.6\\\"` for graph algorithms, `async-trait` for trait implementation, `tokio` for Mutex","acceptance_criteria":"- InMemoryStorage implements all trait methods\n- Uses petgraph for dependency graph\n- Cycle detection working with graph algorithms\n- Ready work calculation via graph traversal\n- All CRUD operations functional\n- Unit tests for all operations\n- Benchmark: 1000 issues in \u003c10ms","notes":"## Clarifications\n\n### Session 2025-11-17\n\n- Q: The task design shows `pub struct InMemoryStorage` but rivets-0gc specifies `InMemoryStorageInner` wrapped in `Arc\u003cMutex\u003c\u003e\u003e`. Which approach should this task implement? → A: Follow rivets-0gc: implement InMemoryStorageInner + Arc\u003cMutex\u003c\u003e\u003e wrapper (ensures thread safety, matches architecture from trait design)\n- Q: The task shows synchronous trait methods (`fn create_issue`) but rivets-0gc defines async trait methods (`async fn create`). How should InMemoryStorage implement the trait? → A: Use blocking operations in async methods (simple for MVP, matches Phase 1 approach from rivets-0gc)","status":"closed","priority":1,"issue_type":"task","assignee":"claude","created_at":"2025-11-17T16:41:41.594099121-06:00","updated_at":"2025-11-17T20:58:07.492885986-06:00","closed_at":"2025-11-17T20:58:07.492885986-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-bz5","depends_on_id":"rivets-0gc","type":"blocks","created_at":"2025-11-17T16:41:41.595147022-06:00","created_by":"daemon"},{"issue_id":"rivets-bz5","depends_on_id":"rivets-x1e","type":"blocks","created_at":"2025-11-17T16:41:41.595557209-06:00","created_by":"daemon"},{"issue_id":"rivets-bz5","depends_on_id":"rivets-06w","type":"blocks","created_at":"2025-11-17T16:46:29.597245914-06:00","created_by":"daemon"}]}
{"id":"rivets-ceg","content_hash":"272209e861b15a998704fb3154c17bd684a8da0891db905060a71c44ff7bfea3","title":"Implement CLI argument parsing with clap validation","description":"Extend the CLI skeleton (rivets-p7v) with complete argument parsing, validation, error handling, and help text generation for all commands.\n\nThis task implements the argument parsing layer that sits between the CLI entry point and command execution. Each command gets proper args structs with validation.\n\n**Commands to implement**:\n- create: --title, --description, --priority, --type, --assignee, --deps\n- list: --status, --priority, --type, --assignee, --label, --limit\n- show: \u003cissue-id\u003e, --json\n- update: \u003cissue-id\u003e, --title, --description, --status, --priority, --assignee\n- close: \u003cissue-id\u003e, --reason\n- delete: \u003cissue-id\u003e, --force\n- init: --prefix, --quiet\n- ready: --assignee, --limit, --sort\n\n**Features**:\n- Validation for all flag values (priority 0-4, valid status enum, etc.)\n- Helpful error messages with suggestions\n- Auto-generated help text via clap derives\n- Subcommand structure with shared global flags\n- Interactive prompts for missing required fields (create command)","design":"Use clap derive API for argument parsing:\n\n```rust\nuse clap::{Parser, Subcommand, ValueEnum};\n\n#[derive(Parser)]\n#[command(name = \"rivets\")]\n#[command(author, version, about, long_about = None)]\npub struct Cli {\n    /// Output in JSON format\n    #[arg(long, global = true)]\n    pub json: bool,\n    \n    #[command(subcommand)]\n    pub command: Option\u003cCommands\u003e,\n}\n\n#[derive(Subcommand)]\npub enum Commands {\n    /// Initialize a new rivets repository\n    Init(InitArgs),\n    \n    /// Create a new issue\n    Create(CreateArgs),\n    \n    /// List issues with optional filters\n    List(ListArgs),\n    \n    /// Show detailed issue information\n    Show(ShowArgs),\n    \n    /// Update an existing issue\n    Update(UpdateArgs),\n    \n    /// Close an issue\n    Close(CloseArgs),\n    \n    /// Delete an issue\n    Delete(DeleteArgs),\n    \n    /// Show ready-to-work issues\n    Ready(ReadyArgs),\n}\n\n#[derive(Parser)]\npub struct CreateArgs {\n    /// Issue title\n    #[arg(short, long)]\n    pub title: Option\u003cString\u003e,\n    \n    /// Issue description\n    #[arg(short, long)]\n    pub description: Option\u003cString\u003e,\n    \n    /// Priority (0-4)\n    #[arg(short, long, value_parser = clap::value_parser!(u8).range(0..=4))]\n    pub priority: Option\u003cu8\u003e,\n    \n    /// Issue type\n    #[arg(short = 't', long)]\n    pub issue_type: Option\u003cIssueTypeArg\u003e,\n    \n    /// Assignee\n    #[arg(short, long)]\n    pub assignee: Option\u003cString\u003e,\n}\n\n#[derive(ValueEnum, Clone)]\npub enum IssueTypeArg {\n    Bug,\n    Feature,\n    Task,\n    Epic,\n    Chore,\n}\n\n// ... more arg structs\n```\n\n**Validation**:\n- Priority range checking (0-4)\n- Issue ID format validation\n- Enum value validation via ValueEnum\n- Required vs optional field handling\n- Custom validators for complex types","acceptance_criteria":"- All CLI commands have argument structs defined\n- Clap derive attributes configured correctly\n- Priority validation enforces 0-4 range\n- Issue ID validation checks format\n- Enum arguments use ValueEnum for type safety\n- Help text auto-generated and informative\n- Error messages are clear with examples\n- Unit tests for argument parsing\n- Integration tests verify --help output\n- Invalid arguments produce helpful error messages","status":"open","priority":1,"issue_type":"task","created_at":"2025-11-17T17:04:11.868192762-06:00","updated_at":"2025-11-17T17:04:11.868192762-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-ceg","depends_on_id":"rivets-p7v","type":"blocks","created_at":"2025-11-17T17:04:17.995576498-06:00","created_by":"daemon"}]}
{"id":"rivets-cgl","content_hash":"bc7742c1d1d7ac8e6c31eb555da4d79c633deff7b59b20aa6e4a63de0dc04ab0","title":"Integrate storage trait with CLI commands","description":"Update CLI commands to use the storage trait instead of direct storage access. Add storage initialization and lifecycle management.","design":"Update rivets CLI to use async storage trait (following rivets-0gc architecture):\n\n**Async CLI Structure:**\n\n```rust\npub struct App {\n    storage: Box\u003cdyn IssueStorage\u003e,\n    config: Config,\n}\n\nimpl App {\n    pub async fn new(config: Config) -\u003e Result\u003cSelf\u003e {\n        let storage = Self::create_storage(\u0026config).await?;\n        Ok(Self { storage, config })\n    }\n    \n    async fn create_storage(config: \u0026Config) -\u003e Result\u003cBox\u003cdyn IssueStorage\u003e\u003e {\n        match config.storage_backend {\n            StorageBackend::InMemory =\u003e {\n                let storage = if let Some(path) = \u0026config.data_file {\n                    if path.exists() {\n                        let (storage, warnings) = InMemoryStorage::load_from_jsonl(path).await?;\n                        if !warnings.is_empty() {\n                            eprintln!(\"Warning: Loaded with {} warnings. Check logs.\", warnings.len());\n                        }\n                        storage\n                    } else {\n                        InMemoryStorage::new()\n                    }\n                } else {\n                    InMemoryStorage::new()\n                };\n                Ok(Box::new(storage))\n            }\n            // Future: PostgreSQL, SQLite\n        }\n    }\n}\n```\n\n**Update IssueStorage trait to include save():**\n\nAdd to rivets-0gc trait definition:\n```rust\n#[async_trait]\npub trait IssueStorage: Send + Sync {\n    // ... existing methods ...\n    \n    /// Persist storage state (for backends that need explicit persistence)\n    async fn save(\u0026self) -\u003e Result\u003c()\u003e;\n}\n```\n\n**Command execution with auto-save:**\n\n```rust\nimpl Commands {\n    pub async fn execute(\u0026self, app: \u0026mut App) -\u003e Result\u003c()\u003e {\n        match self {\n            Commands::Create(args) =\u003e {\n                let issue = app.storage.create(args.into()).await?;\n                app.storage.save().await?;  // Auto-save after mutation\n                println!(\"Created: {}\", issue.id);\n            }\n            Commands::Update(args) =\u003e {\n                let issue = app.storage.update(\u0026args.id, args.into()).await?;\n                app.storage.save().await?;  // Auto-save after mutation\n                println!(\"Updated: {}\", issue.id);\n            }\n            Commands::Close(args) =\u003e {\n                app.storage.delete(\u0026args.id).await?;\n                app.storage.save().await?;  // Auto-save after mutation\n                println!(\"Closed: {}\", args.id);\n            }\n            Commands::Delete(args) =\u003e {\n                app.storage.delete(\u0026args.id).await?;\n                app.storage.save().await?;  // Auto-save after mutation\n                println!(\"Deleted: {}\", args.id);\n            }\n            Commands::List(args) =\u003e {\n                let filter = args.into();\n                let issues = app.storage.list(\u0026filter).await?;\n                // ... display (no save needed for read-only)\n            }\n            Commands::Show(args) =\u003e {\n                let issue = app.storage.get(\u0026args.id).await?;\n                // ... display (no save needed for read-only)\n            }\n            // ... other commands\n        }\n        Ok(())\n    }\n}\n```\n\n**Main.rs with async runtime:**\n\n```rust\n#[tokio::main(flavor = \"current_thread\")]\nasync fn main() -\u003e Result\u003c()\u003e {\n    let cli = Cli::parse();\n    let config = Config::load().await?;\n    let mut app = App::new(config).await?;\n    \n    cli.command.execute(\u0026mut app).await?;\n    \n    Ok(())\n}\n```\n\n**Backend-specific save() implementations:**\n\n```rust\n// InMemoryStorage\n#[async_trait]\nimpl IssueStorage for InMemoryStorage {\n    async fn save(\u0026self) -\u003e Result\u003c()\u003e {\n        if let Some(path) = \u0026self.data_file {\n            self.save_to_jsonl(path).await?;\n        }\n        Ok(())\n    }\n}\n\n// PostgreSQL (future)\n#[async_trait]\nimpl IssueStorage for PostgresStorage {\n    async fn save(\u0026self) -\u003e Result\u003c()\u003e {\n        Ok(())  // No-op: PostgreSQL commits on each operation\n    }\n}\n```\n\n**Key Design Points:**\n- All App methods are async to match storage trait\n- save() added to IssueStorage trait (each backend implements appropriately)\n- Auto-save after every mutating command (create, update, delete, close)\n- Read-only commands (list, show) don't trigger save\n- JSONL load warnings displayed to user\n- Uses tokio current-thread runtime as specified in rivets-0gc","acceptance_criteria":"- CLI uses storage trait, not concrete type\n- Storage initialized on CLI startup\n- Data loaded from JSONL at startup (if exists)\n- Data saved to JSONL on exit\n- All commands work via trait methods\n- Error handling for storage failures\n- Integration tests with InMemoryStorage","notes":"## Clarifications\n\n### Session 2025-11-17\n\n- Q: The storage trait from rivets-0gc is async, but the design shows synchronous App methods. Should the CLI App and command execution be async? → A: Make App and command execution async (matches storage trait pattern, use #[tokio::main] as specified in rivets-0gc)\n- Q: With InMemoryStorage as Arc\u003cMutex\u003cInMemoryStorageInner\u003e\u003e, the design's downcast pattern won't work. How should the CLI trigger save on exit for in-memory backend? → A: Add save() method to IssueStorage trait (trait-based approach, each backend implements appropriately, works with Box\u003cdyn IssueStorage\u003e)\n- Q: When should the in-memory backend persist to JSONL - only on exit, after every command, or periodically? → A: Save after every mutating command (resilient to crashes, better durability guarantee)","status":"open","priority":1,"issue_type":"task","created_at":"2025-11-17T16:41:42.102424622-06:00","updated_at":"2025-11-17T17:29:46.949775376-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-cgl","depends_on_id":"rivets-0gc","type":"blocks","created_at":"2025-11-17T16:46:29.880494761-06:00","created_by":"daemon"},{"issue_id":"rivets-cgl","depends_on_id":"rivets-bz5","type":"blocks","created_at":"2025-11-17T16:46:29.964261698-06:00","created_by":"daemon"},{"issue_id":"rivets-cgl","depends_on_id":"rivets-l66","type":"blocks","created_at":"2025-11-17T17:02:47.898552194-06:00","created_by":"daemon"}]}
{"id":"rivets-cr9","content_hash":"9481cabfce081d1d49c19435f2ae73ae4686d884970b104ae2b2a57f9ece1636","title":"Research beads project and create tasks to implement the project in rust as rivets","description":"We are going to convert the project beads to rust and call it rivets. You should have context to the beads project. Create all needed research and development tasks that you become aware of in beads so that we can begin development","design":"## Beads Research Complete\n\nConducted comprehensive analysis of beads codebase (~19,243 LOC storage layer alone). \n\n**Key Findings**:\n- Hash-based ID generation (SHA256 + base36, adaptive 4-6 chars)\n- 4 dependency types (blocks, related, parent-child, discovered-from)  \n- Recursive CTE algorithms for cycle detection and ready work\n- SQLite with 14 tables, WAL mode, foreign key constraints\n- JSONL import/export with content hash deduplication\n- RPC system (JSON-RPC over Unix sockets)\n- Background daemon with auto-sync\n\n**Created 13 Implementation Tasks**:\n1. rivets-x1e: Hash-based ID generation\n2. rivets-0gc: SQLite storage layer  \n3. rivets-6op: Dependency system with cycle detection\n4. rivets-qeb: Ready work algorithm\n5. rivets-x51: JSONL import/export\n6. rivets-bsp: Core CLI commands\n7. rivets-xy9: Configuration system\n8. rivets-bi2: RPC system\n9. rivets-9mh: Daemon process\n10. rivets-37y: Labels and comments\n11. rivets-6tl: Filtering and queries\n12. rivets-4l2: Init command\n13. rivets-azn: Architecture documentation\n\nFull analysis documented in agent output.","acceptance_criteria":"✓ Beads codebase explored thoroughly\n✓ Core features documented (ID generation, dependencies, ready work, etc.)\n✓ Key algorithms identified (cycle detection, hash IDs, recursive CTEs)\n✓ 13 implementation tasks created in beads\n✓ Tasks cover all major beads functionality\n✓ Each task has design notes from beads source","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2025-11-17T15:01:56.182254064-06:00","updated_at":"2025-11-17T16:17:10.318274001-06:00","closed_at":"2025-11-17T16:17:10.318274001-06:00","source_repo":"."}
{"id":"rivets-cxq","content_hash":"b9a4c3673daea50b8eb9817d2f3c3d7df70de1ba7608aed60e9a0c727aa698bc","title":"Add priority constants (M-DOCUMENTED-MAGIC)","description":"Priority validation in domain/mod.rs uses magic number `4` without named constants, violating the M-DOCUMENTED-MAGIC guideline.\n\nLocations:\n- Line 133: `if self.priority \u003e 4`\n- Line 269: `if self.priority \u003e 4`\n- Line 500-512: Test using raw numbers 0..=4\n\nCurrent Issues:\n- Magic number repeated in multiple places\n- No single source of truth for valid priority range\n- Unclear to readers what the valid range is","design":"Add public constants in domain/mod.rs after MAX_TITLE_LENGTH (line 207):\n\n```rust\n/// Minimum priority value (highest urgency)\npub const MIN_PRIORITY: u8 = 0;\n\n/// Maximum priority value (lowest urgency)\npub const MAX_PRIORITY: u8 = 4;\n```\n\nUpdate validation code:\n\n```rust\nif self.priority \u003e MAX_PRIORITY {\n    return Err(format!(\n        \"Priority must be in range {}-{} (got {})\",\n        MIN_PRIORITY, MAX_PRIORITY, self.priority\n    ));\n}\n```\n\nUpdate tests to use constants.","acceptance_criteria":"- [ ] MIN_PRIORITY and MAX_PRIORITY constants defined\n- [ ] All validation code uses constants instead of magic numbers\n- [ ] Error messages reference constants\n- [ ] Tests use constants (e.g., `for priority in MIN_PRIORITY..=MAX_PRIORITY`)\n- [ ] Constants are documented\n- [ ] All tests pass","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-27T16:48:58.565187568-06:00","updated_at":"2025-11-27T16:48:58.565187568-06:00","source_repo":".","labels":["code-quality","documentation","refactoring"]}
{"id":"rivets-d71","content_hash":"7fdcc106230845f1449bd8305ea03e56f3271bd37b7a5d57a0c1fc7748fadb04","title":"Implement PostgreSQL storage backend with recursive CTEs","description":"Create PostgresStorage implementation using sqlx with connection pooling. Use recursive CTEs for cycle detection and ready work queries. This is the production multi-user backend.","design":"Implement PostgreSQL backend:\n\n```rust\npub struct PostgresStorage {\n    pool: PgPool,\n}\n\nimpl IssueStorage for PostgresStorage {\n    fn create_issue(\u0026mut self, new: NewIssue) -\u003e Result\u003cIssue\u003e {\n        let id = generate_hash_id(\u0026new);\n        sqlx::query!(\n            \"INSERT INTO issues (id, title, description, ...) VALUES ($1, $2, $3, ...)\",\n            id, new.title, new.description\n        ).execute(\u0026self.pool).await?;\n        \n        self.get_issue(\u0026id)?.ok_or(Error::NotFound)\n    }\n    \n    fn check_cycle(\u0026self, from: \u0026IssueId, to: \u0026IssueId) -\u003e Result\u003cbool\u003e {\n        let exists = sqlx::query_scalar!(\n            r#\"\n            WITH RECURSIVE paths AS (\n                SELECT issue_id, depends_on_id, 1 as depth\n                FROM dependencies WHERE issue_id = $1\n                UNION ALL\n                SELECT d.issue_id, d.depends_on_id, p.depth + 1\n                FROM dependencies d JOIN paths p \n                ON d.issue_id = p.depends_on_id\n                WHERE p.depth \u003c 100\n            )\n            SELECT EXISTS(SELECT 1 FROM paths WHERE depends_on_id = $2)\n            \"#,\n            to, from\n        ).fetch_one(\u0026self.pool).await?;\n        \n        Ok(exists.unwrap_or(false))\n    }\n    \n    fn find_ready(\u0026self, filter: \u0026IssueFilter) -\u003e Result\u003cVec\u003cIssue\u003e\u003e {\n        // Recursive CTE from beads ready.go\n        // ... implementation\n    }\n}\n```\n\n**Schema**: Similar to beads SQLite schema, optimized for PostgreSQL\n**Dependencies**: `sqlx` with `runtime-tokio-rustls`, `postgres` features\n**Migrations**: Use sqlx migrations","acceptance_criteria":"- PostgresStorage implements all trait methods\n- Connection pooling configured\n- Recursive CTEs for cycle detection and ready work\n- All CRUD operations functional\n- Indexes for performance\n- Migration system working\n- Integration tests with test database\n- Performance: \u003c50ms for complex queries","status":"open","priority":3,"issue_type":"task","created_at":"2025-11-17T16:41:42.669213538-06:00","updated_at":"2025-11-17T16:41:42.669213538-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-d71","depends_on_id":"rivets-0gc","type":"blocks","created_at":"2025-11-17T16:41:42.670260497-06:00","created_by":"daemon"}]}
{"id":"rivets-dgt","content_hash":"91504b1b9443e44e83a9c6d2318cfcd2362876c78bdf265f7426315d9f68b1a4","title":"Define Warning types and collection system","description":"Define the Warning enum and WarningCollector for tracking non-fatal errors during JSONL loading. This enables resilient loading that continues despite malformed data.\n\nBased on the research API design and existing LoadWarning pattern from in_memory.rs.","design":"```rust\n#[derive(Debug, Clone)]\npub enum Warning {\n    MalformedJson { line_number: usize, error: String },\n    SkippedLine { line_number: usize, reason: String },\n}\n\npub struct WarningCollector {\n    warnings: Arc\u003cMutex\u003cVec\u003cWarning\u003e\u003e\u003e,\n}\n\nimpl WarningCollector {\n    pub fn new() -\u003e Self {\n        Self {\n            warnings: Arc::new(Mutex::new(Vec::new())),\n        }\n    }\n    \n    pub fn add(\u0026self, warning: Warning) {\n        self.warnings.lock().unwrap().push(warning);\n    }\n    \n    pub fn into_warnings(self) -\u003e Vec\u003cWarning\u003e {\n        Arc::try_unwrap(self.warnings)\n            .unwrap_or_else(|arc| (*arc.lock().unwrap()).clone())\n    }\n}\n```","acceptance_criteria":"- Warning enum defined with MalformedJson and SkippedLine variants\n- WarningCollector struct with thread-safe add()\n- into_warnings() method to extract collected warnings\n- Clone and Debug impls for Warning\n- Unit tests for warning collection\n- Compiles without errors","status":"open","priority":1,"issue_type":"task","created_at":"2025-11-27T17:15:49.553769651-06:00","updated_at":"2025-11-27T17:15:49.553769651-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-dgt","depends_on_id":"rivets-3r7","type":"blocks","created_at":"2025-11-27T17:19:21.272459877-06:00","created_by":"dwalleck"}]}
{"id":"rivets-dhs","content_hash":"895dddb8834cb7e4ed7cb1ccf4887abbd1620c8b4d6bee6482109cc0e2c02a85","title":"Add query performance benchmarks","description":"Create benchmarks to measure query/filter performance and verify the \"1M records in \u003c5s\" target from research.\n\nUses criterion for benchmarking.","design":"Create benches/query_benchmarks.rs:\n\n```rust\nuse criterion::{criterion_group, criterion_main, Criterion, BenchmarkId};\nuse rivets_jsonl::*;\n\nfn query_benchmarks(c: \u0026mut Criterion) {\n    let mut group = c.benchmark_group(\"query\");\n    \n    // Benchmark: filter 1M records\n    group.bench_function(\"filter_1m_records\", |b| {\n        b.iter(|| {\n            // ... benchmark code\n        });\n    });\n    \n    // Benchmark: filter + map pipeline\n    group.bench_function(\"filter_map_pipeline\", |b| {\n        b.iter(|| {\n            // ... benchmark code\n        });\n    });\n    \n    // Benchmark: complex multi-predicate filter\n    group.bench_function(\"complex_filter\", |b| {\n        b.iter(|| {\n            // ... benchmark code\n        });\n    });\n}\n\ncriterion_group!(benches, query_benchmarks);\ncriterion_main!(benches);\n```\n\nAdd criterion as dev-dependency.","acceptance_criteria":"- Benchmarks created in benches/\n- Measures filter performance\n- Measures map performance  \n- Measures complex query pipelines\n- 1M records benchmark runs\n- Results documented in comments\n- cargo bench runs successfully","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-27T17:17:06.817308399-06:00","updated_at":"2025-11-27T17:17:06.817308399-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-dhs","depends_on_id":"rivets-6p4","type":"blocks","created_at":"2025-11-27T17:20:07.101126653-06:00","created_by":"dwalleck"},{"issue_id":"rivets-dhs","depends_on_id":"rivets-08u","type":"blocks","created_at":"2025-11-27T17:20:12.837887431-06:00","created_by":"dwalleck"}]}
{"id":"rivets-dve","content_hash":"864a4b9893d6705cd56cf136140f84e6c4e60b0afcca6c011f210986ef1a905f","title":"Add test-util feature for MockStorage (M-TEST-UTIL)","description":"MockStorage in storage/mod.rs (lines 284-390) should be feature-gated to allow downstream crates to use it for testing their own code that depends on the IssueStorage trait.\n\nCurrent State:\n- MockStorage is only available with #[cfg(test)]\n- Downstream crates cannot access it for their own tests\n\nImpact:\n- Makes library harder to use for external consumers who need to test code using IssueStorage\n- Forces users to write their own mocks","design":"Add a `test-util` feature to Cargo.toml:\n\n```toml\n[features]\ntest-util = []\n```\n\nMove MockStorage to public API under feature gate:\n\n```rust\n#[cfg(any(test, feature = \"test-util\"))]\npub struct MockStorage;\n\n#[cfg(any(test, feature = \"test-util\"))]\nimpl MockStorage {\n    pub fn new() -\u003e Self { Self }\n}\n```\n\nUpdate documentation to explain the feature and its usage.","acceptance_criteria":"- [ ] `test-util` feature added to Cargo.toml\n- [ ] MockStorage is `pub` and available with `feature = \"test-util\"`\n- [ ] MockStorage includes constructor and helper methods\n- [ ] Documentation updated with usage example\n- [ ] Existing tests still pass","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-27T16:48:52.858336885-06:00","updated_at":"2025-11-27T16:48:52.858336885-06:00","source_repo":".","labels":["code-quality","public-api","testing"]}
{"id":"rivets-fk9","content_hash":"f81f4504609749d2ae904450922cfb215474b0b8d3fe0bd90b7a579702e0298b","title":"Research and design JSONL library architecture","description":"Design a standalone Rust library for efficient JSONL (JSON Lines) operations that can be used by rivets and potentially other projects. The library should handle reading, writing, querying, and streaming JSONL data structures.","design":"Research key features needed:\n- Efficient line-by-line reading and writing\n- Streaming support for large files\n- Querying/filtering capabilities\n- Schema validation (optional)\n- Error handling for malformed JSON\n- Memory-efficient operations\n- Concurrent access patterns\n\nConsider existing Rust JSONL libraries and what gaps exist. Determine if we build from scratch or extend existing solutions.","acceptance_criteria":"- Document listing key features and API design\n- Comparison of existing Rust JSONL libraries (evaluated on: performance benchmarks, API ergonomics, maintenance status, license compatibility, feature completeness)\n- Decision matrix showing trade-offs\n- Decision on whether to build new library or extend existing one\n- Performance requirements documented (targets: streaming 100MB file \u003c1s, memory usage \u003c10MB regardless of file size)\n- Public API surface designed with examples\n- Compatibility with serde ecosystem verified","notes":"## Research Complete (2025-11-27)\n\nComprehensive research documented in `docs/rivets-jsonl-research.md`.\n\n**Key Findings**:\n- Evaluated 4 existing Rust JSONL libraries (jsonl, serde-jsonlines, json-lines, json-stream)\n- None fully meet rivets' needs (async-first, streaming queries, resilient loading)\n- **Decision**: Build custom implementation borrowing proven patterns\n\n**Performance Targets**:\n- 100MB file in \u003c1s (read)\n- \u003c10MB memory regardless of file size\n- Match or exceed current in_memory.rs baseline\n\n**API Design**:\n- Async-native with tokio\n- Extension traits for ergonomics\n- Streaming via futures::Stream\n- Resilient loading with warning collection\n- Query/filter during stream\n\n**Next**: Implement Phase 1 (Core Read/Write)","status":"closed","priority":1,"issue_type":"task","assignee":"claude","created_at":"2025-11-17T15:08:44.178736229-06:00","updated_at":"2025-11-27T17:01:29.152853238-06:00","closed_at":"2025-11-27T17:01:29.152853238-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-fk9","depends_on_id":"rivets-cr9","type":"blocks","created_at":"2025-11-17T15:08:44.179627634-06:00","created_by":"daemon"}]}
{"id":"rivets-kr3","content_hash":"5812bc8743423a9af5f2fb2197962df0277b0570a28445978b34108bcf2ff036","title":"Design rivets project structure with two-crate architecture","description":"Plan the overall project structure splitting rivets into two crates: a JSONL library (general-purpose) and the rivets CLI application (issue tracking). Define workspace layout, dependencies, and inter-crate relationships.","design":"## Workspace Design\n\n### Directory Structure\n```\nrivets/\n├── Cargo.toml              # Workspace config (resolver=3)\n├── crates/\n│   ├── rivets-jsonl/       # JSONL library crate\n│   │   ├── src/ (lib.rs, reader.rs, writer.rs, query.rs, stream.rs, error.rs)\n│   │   ├── tests/, benches/, examples/\n│   │   └── Cargo.toml\n│   └── rivets/             # CLI application crate\n│       ├── src/ (main.rs, cli.rs, commands/, domain/, storage.rs, config.rs)\n│       ├── tests/\n│       └── Cargo.toml\n├── docs/\n└── target/                 # Shared build output\n```\n\n### Separation of Concerns\n\n**rivets-jsonl**: Generic JSONL operations (read, write, stream, query), no domain knowledge\n**rivets**: Issue tracking domain (CLI, commands, business logic), uses rivets-jsonl for storage\n\n### Dependencies\n- One-way: rivets → rivets-jsonl\n- Shared deps via [workspace.dependencies]: serde, serde_json, thiserror, anyhow, clap\n- rivets-jsonl internally referenced via { workspace = true }\n\n### Naming\n- Library: rivets-jsonl (crates.io)\n- Binary: rivets (crates.io), installs 'rivets' command\n- Modules: snake_case, one primary type per file\n\n### Testing\n- Unit: #[cfg(test)] modules\n- Integration: crates/*/tests/\n- Benchmarks: crates/rivets-jsonl/benches/\n- Doc tests: /// examples in public APIs\n\n### Future Extensibility\nWorkspace supports: rivets-tui, rivets-server, rivets-web, rivets-sync","acceptance_criteria":"- Cargo.toml workspace configuration designed\n- Directory structure planned and documented\n- Clear separation of concerns between crates defined\n- Naming conventions established\n- Inter-crate dependency strategy documented","notes":"Design complete with comprehensive workspace structure. Ready for implementation. Directory structure uses flat 'crates/' layout per Rust best practices. Resolver 3 for modern dependency resolution. Clean separation between generic JSONL library and domain-specific CLI application.","status":"closed","priority":1,"issue_type":"task","assignee":"Claude","created_at":"2025-11-17T15:08:45.002094002-06:00","updated_at":"2025-11-17T15:47:19.541529556-06:00","closed_at":"2025-11-17T15:43:27.115215646-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-kr3","depends_on_id":"rivets-cr9","type":"blocks","created_at":"2025-11-17T15:08:45.002895148-06:00","created_by":"daemon"}]}
{"id":"rivets-kuf","content_hash":"831b7e59dca2abe251fe64d5e8165b4c52a9f8b5840ec5fef8d6bfb7f83f11ef","title":"Create comprehensive examples","description":"Create examples demonstrating all major rivets-jsonl features.\n\nExamples should be runnable and well-documented.","design":"Create in examples/:\n\n1. **basic.rs** - Simple read/write\n2. **streaming.rs** - Large file streaming\n3. **resilient.rs** - Resilient loading with warnings\n4. **query.rs** - Filtering and querying\n5. **atomic_write.rs** - Atomic writes\n6. **integration.rs** - Integration with rivets\n\nEach example should:\n- Be fully runnable with `cargo run --example \u003cname\u003e`\n- Include inline documentation\n- Demonstrate best practices\n- Show error handling","acceptance_criteria":"- All 6 examples created\n- Each example compiles and runs\n- Examples well-documented\n- README lists all examples with descriptions\n- Examples demonstrate key features","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-27T17:17:58.239786888-06:00","updated_at":"2025-11-27T17:17:58.239786888-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-kuf","depends_on_id":"rivets-t0k","type":"blocks","created_at":"2025-11-27T17:20:30.06094014-06:00","created_by":"dwalleck"},{"issue_id":"rivets-kuf","depends_on_id":"rivets-dhs","type":"blocks","created_at":"2025-11-27T17:20:35.736904071-06:00","created_by":"dwalleck"}]}
{"id":"rivets-l56","content_hash":"4ed459de22c265b7b14d0294f7e7fe1c44647b287892bce5486da8e1efa8effe","title":"Replace #[allow] with #[expect] (M-LINT-OVERRIDE-EXPECT)","description":"storage/mod.rs:233 uses `#[allow(dead_code)]` on the PostgreSQL enum variant, violating the M-LINT-OVERRIDE-EXPECT guideline.\n\nCurrent Code (line 233):\n```rust\n#[allow(dead_code)]\nPostgreSQL(String),\n```\n\nIssues:\n- #[allow] silently suppresses warnings\n- No explanation for why the warning is acceptable\n- Will continue to suppress warning even if PostgreSQL gets implemented\n\nGuideline Requirement:\n- Use #[expect] with reason to document intent\n- Fails loudly if expectation becomes invalid","design":"Replace #[allow] with #[expect] and add reason:\n\n```rust\n#[expect(dead_code, reason = \"PostgreSQL backend not yet implemented (tracked in TODO)\")]\nPostgreSQL(String),\n```\n\nThis:\n- Documents why the variant is currently unused\n- Will cause a warning if PostgreSQL gets implemented but attribute isn't removed\n- Makes the temporary nature explicit","acceptance_criteria":"- [ ] #[allow(dead_code)] replaced with #[expect]\n- [ ] Reason documented explaining temporary nature\n- [ ] Code compiles without warnings\n- [ ] All tests pass","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-27T16:49:04.233675714-06:00","updated_at":"2025-11-27T16:49:04.233675714-06:00","source_repo":".","labels":["code-quality","lints"]}
{"id":"rivets-l66","content_hash":"656b27cd18c016d01f4a2da6d824e546aab6f7730aded3bfcaf1a3a99b029c67","title":"Implement JSONL persistence for InMemoryStorage","description":"Add save/load functionality to InMemoryStorage to persist data to JSONL files. This provides durability for the in-memory backend.","design":"Extend InMemoryStorage with async persistence (matches rivets-0gc async trait pattern):\n\n**Async Implementation:**\n\n```rust\nimpl InMemoryStorage {\n    pub async fn load_from_jsonl(path: \u0026Path) -\u003e Result\u003c(Self, Vec\u003cLoadWarning\u003e)\u003e {\n        use tokio::fs::File;\n        use tokio::io::{AsyncBufReadExt, BufReader};\n        \n        let file = BufReader::new(File::open(path).await?);\n        let storage = Arc::new(Mutex::new(InMemoryStorageInner::new()));\n        let mut warnings = Vec::new();\n        let mut lines = file.lines();\n        \n        // First pass: import all issues (without dependencies)\n        while let Some(line) = lines.next_line().await? {\n            match serde_json::from_str::\u003cIssue\u003e(\u0026line) {\n                Ok(issue) =\u003e {\n                    let mut inner = storage.lock().await;\n                    let id = issue.id.clone();\n                    inner.issues.insert(id.clone(), issue);\n                    let node = inner.graph.add_node(id.clone());\n                    inner.node_map.insert(id, node);\n                }\n                Err(e) =\u003e {\n                    warnings.push(LoadWarning::MalformedJson { line, error: e });\n                    log::warn!(\"Skipping malformed JSON: {}\", e);\n                }\n            }\n        }\n        \n        // Second pass: add dependency edges with orphan/cycle detection\n        let mut inner = storage.lock().await;\n        for issue in inner.issues.values() {\n            for dep in \u0026issue.dependencies {\n                // Check if dependency target exists\n                if !inner.issues.contains_key(\u0026dep.depends_on_id) {\n                    warnings.push(LoadWarning::OrphanedDependency {\n                        from: issue.id.clone(),\n                        to: dep.depends_on_id.clone(),\n                    });\n                    log::warn!(\"Skipping orphaned dependency: {} -\u003e {}\", \n                              issue.id, dep.depends_on_id);\n                    continue;\n                }\n                \n                // Check for cycles before adding edge\n                if inner.has_cycle_internal(\u0026issue.id, \u0026dep.depends_on_id)? {\n                    warnings.push(LoadWarning::CircularDependency {\n                        from: issue.id.clone(),\n                        to: dep.depends_on_id.clone(),\n                    });\n                    log::warn!(\"Skipping circular dependency: {} -\u003e {}\", \n                              issue.id, dep.depends_on_id);\n                    continue;\n                }\n                \n                // Add edge to graph\n                inner.add_dependency_edge_internal(dep)?;\n            }\n        }\n        \n        Ok((storage, warnings))\n    }\n    \n    pub async fn save_to_jsonl(\u0026self, path: \u0026Path) -\u003e Result\u003c()\u003e {\n        use tokio::fs::File;\n        use tokio::io::{AsyncWriteExt, BufWriter};\n        \n        // Atomic write: temp file + rename\n        let temp_path = path.with_extension(\"tmp\");\n        let file = BufWriter::new(File::create(\u0026temp_path).await?);\n        \n        let inner = self.lock().await;\n        for issue in inner.issues.values() {\n            let json = serde_json::to_string(issue)?;\n            file.write_all(json.as_bytes()).await?;\n            file.write_all(b\"\\n\").await?;  // Newline-delimited\n        }\n        file.flush().await?;\n        \n        // Atomic rename\n        tokio::fs::rename(\u0026temp_path, path).await?;\n        Ok(())\n    }\n}\n```\n\n**Error Handling:**\n- Malformed JSON: Skip line, log warning, continue loading\n- Orphaned dependencies: Skip edge, log warning, import issue without that dependency\n- Circular dependencies: Skip edge, log warning, prevent cycle creation\n- Returns (Storage, Vec\u003cLoadWarning\u003e) to communicate warnings to caller\n\n**File format**: Standard JSONL (newline-delimited JSON)\n**Atomicity**: Write to temp file, then rename (atomic on POSIX systems)\n**Dependencies**: tokio with fs feature for async file I/O","acceptance_criteria":"- load_from_jsonl reads and reconstructs storage\n- save_to_jsonl writes all issues\n- Dependency graph correctly rebuilt on load\n- Atomic writes (temp file + rename)\n- Round-trip test: save → load → verify\n- Handles large files (streaming read)\n- Error handling for malformed JSON","notes":"## Clarifications\n\n### Session 2025-11-17\n\n- Q: The design shows synchronous methods (`pub fn load_from_jsonl`, `pub fn save_to_jsonl`) but rivets-0gc defines an async storage trait. Should these persistence methods be async? → A: Make persistence methods async (matches async trait pattern from rivets-0gc, enables non-blocking file I/O for large JSONL files)\n- Q: During JSONL import, what should happen if an issue references dependencies that don't exist in the file (orphaned dependency references)? → A: Skip orphaned dependencies with warning (allows partial recovery from corrupted/incomplete files, resilient approach)\n- Q: If the JSONL file contains issues that form circular dependencies, how should load_from_jsonl handle this? → A: Detect and reject circular dependencies during import (ensures loaded data has no cycles, maintains data integrity)","status":"closed","priority":1,"issue_type":"task","assignee":"claude","created_at":"2025-11-17T16:41:41.842178005-06:00","updated_at":"2025-11-27T16:55:23.560905657-06:00","closed_at":"2025-11-27T16:55:23.560905657-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-l66","depends_on_id":"rivets-bz5","type":"blocks","created_at":"2025-11-17T16:46:29.806550029-06:00","created_by":"daemon"}]}
{"id":"rivets-len","content_hash":"407ab354fce286d6f5cf2e7bd9e9b1fef012d5793dd84476b524464f6e6c64d8","title":"Implement JsonlReader::read_line() method","description":"Implement the read_line() async method that reads a single JSONL line and deserializes it into type T.\n\nThis is the foundational read operation that all other reading functionality builds on.","design":"```rust\nuse serde::de::DeserializeOwned;\nuse tokio::io::AsyncBufReadExt;\n\nimpl\u003cR: AsyncRead + Unpin\u003e JsonlReader\u003cR\u003e {\n    pub async fn read_line\u003cT: DeserializeOwned\u003e(\u0026mut self) -\u003e Result\u003cOption\u003cT\u003e\u003e {\n        let mut line = String::new();\n        let bytes_read = self.reader.read_line(\u0026mut line).await?;\n        \n        if bytes_read == 0 {\n            return Ok(None); // EOF\n        }\n        \n        self.line_number += 1;\n        \n        let trimmed = line.trim();\n        if trimmed.is_empty() {\n            // Skip empty lines, read next\n            return self.read_line().await;\n        }\n        \n        let value: T = serde_json::from_str(trimmed)\n            .map_err(|e| Error::Json(e))?;\n        \n        Ok(Some(value))\n    }\n}\n```","acceptance_criteria":"- read_line() method implemented\n- Returns Result\u003cOption\u003cT\u003e\u003e (None for EOF)\n- Increments line_number on each line\n- Skips empty lines\n- Deserializes JSON using serde_json\n- Proper error handling for malformed JSON\n- Unit tests verify reading single/multiple lines\n- Unit tests verify EOF handling","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T17:14:40.175422639-06:00","updated_at":"2025-11-27T18:05:52.845896777-06:00","closed_at":"2025-11-27T18:05:52.845896777-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-len","depends_on_id":"rivets-uo7","type":"blocks","created_at":"2025-11-27T17:18:46.616472736-06:00","created_by":"dwalleck"}]}
{"id":"rivets-p7v","content_hash":"fb1cffffd0c6fb9671ec97213ec54b966cd33de6e80b0d6e9bf37d24ab45afff","title":"Implement rivets CLI skeleton with basic command structure","description":"Create the initial implementation of the rivets CLI application including main.rs, CLI argument parsing with clap, and stub modules for commands and domain logic. Create a hello-world style application that compiles and runs as `bd`.","design":"Based on rivets-kr3 design:\n- Create src/main.rs with CLI entry point\n- Create src/cli.rs with clap-based argument parsing\n- Create src/commands/ module with stub command implementations\n- Create src/domain/ module for issue tracking domain types\n- Create src/storage.rs stub for storage layer\n- Create src/config.rs stub for configuration\n- Create src/error.rs for CLI-specific errors\n- Ensure binary is named 'rivets' in Cargo.toml\n- Add basic integration test that runs the CLI","acceptance_criteria":"- Application compiles and runs as `rivets` command\n- CLI argument parsing works with clap\n- Help message displays correctly with `rivets --help`\n- All module stubs created with proper exports\n- Basic integration test runs the CLI successfully\n- `cargo test --package rivets` passes\n- `cargo run --package rivets` executes without errors","status":"closed","priority":1,"issue_type":"task","assignee":"Claude","created_at":"2025-11-17T15:45:33.081597765-06:00","updated_at":"2025-11-17T16:08:32.126908789-06:00","closed_at":"2025-11-17T16:08:32.126908789-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-p7v","depends_on_id":"rivets-kr3","type":"blocks","created_at":"2025-11-17T15:45:33.08312916-06:00","created_by":"daemon"}]}
{"id":"rivets-qeb","content_hash":"7730bea70c929299f43c2eac44eb055de7d1dc41c9c2130faea4e3a3179ea0fc","title":"Implement ready work algorithm with recursive blocking","description":"Implement the ready work detection algorithm that finds unblocked issues by recursively propagating blocks through parent-child hierarchies using CTEs.","design":"Based on beads ready.go, adapted for Phase 1 (in-memory + petgraph):\n\n**Algorithm** (Phase 1 - petgraph):\n1. Find directly blocked issues (via 'blocks' deps to open/in_progress issues)\n2. Recursively propagate blockage through parent-child deps (depth limit 50)\n3. Exclude all blocked issues from results\n\n**Phase 1 Implementation**:\n```rust\nimpl InMemoryStorage {\n    fn find_ready(\u0026self, filter: Option\u003c\u0026IssueFilter\u003e) -\u003e Result\u003cVec\u003cIssue\u003e\u003e {\n        use petgraph::Direction;\n        \n        // Find all blocked issues\n        let mut blocked = HashSet::new();\n        \n        // Direct blocks: issues with blocking dependencies\n        for (id, issue) in \u0026self.issues {\n            if issue.status == Status::Closed {\n                continue;\n            }\n            \n            for dep in \u0026issue.dependencies {\n                if dep.dep_type == DependencyType::Blocks {\n                    let blocker = self.issues.get(\u0026dep.depends_on_id)?;\n                    if blocker.status == Status::Open || blocker.status == Status::InProgress {\n                        blocked.insert(id.clone());\n                    }\n                }\n            }\n        }\n        \n        // Transitive blocking via parent-child (BFS with depth limit)\n        let mut to_process: VecDeque\u003c(IssueId, usize)\u003e = blocked.iter()\n            .map(|id| (id.clone(), 0))\n            .collect();\n        \n        while let Some((id, depth)) = to_process.pop_front() {\n            if depth \u003e= 50 { continue; }\n            \n            // Find children (issues that depend on this one via parent-child)\n            let node = self.node_map.get(\u0026id)?;\n            for edge in self.graph.edges_directed(*node, Direction::Incoming) {\n                if edge.weight() == \u0026DependencyType::ParentChild {\n                    let child_node = edge.source();\n                    let child_id = \u0026self.graph[child_node];\n                    if blocked.insert(child_id.clone()) {\n                        to_process.push_back((child_id.clone(), depth + 1));\n                    }\n                }\n            }\n        }\n        \n        // Filter out blocked issues\n        let mut ready: Vec\u003cIssue\u003e = self.issues.values()\n            .filter(|issue| {\n                issue.status != Status::Closed \n                \u0026\u0026 !blocked.contains(\u0026issue.id)\n            })\n            .cloned()\n            .collect();\n        \n        // Apply additional filter if provided\n        if let Some(filter) = filter {\n            ready = self.apply_filter(ready, filter)?;\n        }\n        \n        // Sort by policy (hybrid default)\n        self.sort_by_policy(\u0026mut ready, SortPolicy::Hybrid);\n        \n        Ok(ready)\n    }\n}\n```\n\n**Sort Policies**:\n- `hybrid` (default): Recent (48h) by priority, older by age\n- `priority`: P0→P1→P2→P3→P4 strict\n- `oldest`: Creation date ascending\n\n**Phase 3 (PostgreSQL)**: Will use recursive CTEs for blocking propagation (see design notes)","acceptance_criteria":"- Ready issues query excludes blocked work using petgraph\n- Recursive parent-child blocking via BFS traversal\n- All 3 sort policies implemented\n- Depth limit (50) prevents infinite loops\n- Performance: \u003c10ms for 1000 issues\n- Unit tests for complex blocking scenarios\n- Integration test with various dependency graphs","status":"open","priority":1,"issue_type":"task","created_at":"2025-11-17T16:15:21.592668862-06:00","updated_at":"2025-11-17T17:03:27.421386954-06:00","source_repo":"."}
{"id":"rivets-t0k","content_hash":"3a0fe1fe3d74e51d456d0164ba785bc4fdfe365b0ac46b4cefcaeadd66d5759c","title":"Add comprehensive tests for resilient loading","description":"Create comprehensive tests for Phase 2 resilient loading functionality.\n\nTests should verify warning collection, error recovery, and integration with in_memory storage.","design":"Test categories:\n\n**Warning collection tests**:\n- Collect warnings for malformed JSON\n- Collect warnings for skipped lines\n- Warning contains correct line numbers\n- Multiple warnings collected\n\n**Resilient streaming tests**:\n- stream_resilient() continues on errors\n- stream_resilient() yields only valid records\n- Mixed valid/invalid records handled correctly\n\n**Integration tests**:\n- read_jsonl_resilient() with corrupted file\n- in_memory::load_from_jsonl with warnings\n- Verify existing in_memory tests still pass","acceptance_criteria":"- Unit tests for warning collection\n- Unit tests for stream_resilient()\n- Integration tests for read_jsonl_resilient()\n- Integration tests for in_memory integration\n- All tests pass\n- Test coverage \u003e80% for Phase 2 code","status":"open","priority":1,"issue_type":"task","created_at":"2025-11-27T17:16:12.327924224-06:00","updated_at":"2025-11-27T17:16:12.327924224-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-t0k","depends_on_id":"rivets-4q2","type":"blocks","created_at":"2025-11-27T17:19:44.169696383-06:00","created_by":"dwalleck"}]}
{"id":"rivets-uo7","content_hash":"42110ef2eb5cf61fff2afa7aba6e5c513ad7b8d453869632cb60de48f969fb50","title":"Define core JsonlReader and JsonlWriter types","description":"Define the core async types for JsonlReader and JsonlWriter with proper generic parameters and trait bounds. This establishes the foundation for all read/write operations.\n\nBased on research API design:\n- JsonlReader\u003cR: AsyncRead + Unpin\u003e\n- JsonlWriter\u003cW: AsyncWrite + Unpin\u003e\n- Both wrap BufReader/BufWriter for buffering","design":"```rust\nuse tokio::io::{AsyncRead, AsyncWrite, BufReader, BufWriter};\n\npub struct JsonlReader\u003cR\u003e {\n    reader: BufReader\u003cR\u003e,\n    line_number: usize,\n}\n\nimpl\u003cR: AsyncRead + Unpin\u003e JsonlReader\u003cR\u003e {\n    pub fn new(reader: R) -\u003e Self {\n        Self {\n            reader: BufReader::new(reader),\n            line_number: 0,\n        }\n    }\n}\n\npub struct JsonlWriter\u003cW\u003e {\n    writer: BufWriter\u003cW\u003e,\n}\n\nimpl\u003cW: AsyncWrite + Unpin\u003e JsonlWriter\u003cW\u003e {\n    pub fn new(writer: W) -\u003e Self {\n        Self {\n            writer: BufWriter::new(writer),\n        }\n    }\n}\n```\n\nAdd tokio dependency with features = [\"io-util\", \"fs\"].","acceptance_criteria":"- JsonlReader struct defined with generic R: AsyncRead + Unpin\n- JsonlWriter struct defined with generic W: AsyncWrite + Unpin\n- Both have new() constructors\n- BufReader/BufWriter used for buffering\n- Line number tracking in JsonlReader\n- Compiles without errors\n- Basic struct tests pass","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T17:14:00.581453653-06:00","updated_at":"2025-11-27T17:46:18.409886819-06:00","closed_at":"2025-11-27T17:46:18.409886819-06:00","source_repo":".","labels":["phase-1","rivets-jsonl"]}
{"id":"rivets-uyg","content_hash":"2e34ca72834672fff8450eda1cc77c46be6606e322976c7be08d9af50340c254","title":"Implement read_jsonl_resilient() convenience function","description":"Implement the read_jsonl_resilient() convenience function that reads an entire JSONL file into a Vec while collecting warnings.\n\nThis provides an easy-to-use API for resilient loading from file paths.","design":"```rust\nuse std::path::Path;\nuse tokio::fs::File;\nuse futures::StreamExt;\n\npub async fn read_jsonl_resilient\u003cT, P\u003e(\n    path: P\n) -\u003e Result\u003c(Vec\u003cT\u003e, Vec\u003cWarning\u003e)\u003e\nwhere\n    T: DeserializeOwned + 'static,\n    P: AsRef\u003cPath\u003e,\n{\n    let file = File::open(path).await?;\n    let reader = JsonlReader::new(file);\n    let (stream, collector) = reader.stream_resilient();\n    \n    let values: Vec\u003cT\u003e = stream.collect().await;\n    let warnings = collector.into_warnings();\n    \n    Ok((values, warnings))\n}\n```","acceptance_criteria":"- read_jsonl_resilient() function implemented\n- Returns Result\u003c(Vec\u003cT\u003e, Vec\u003cWarning\u003e)\u003e\n- Reads entire file into memory\n- Collects all warnings\n- Integration test with corrupted JSONL file\n- Verifies warnings contain correct line numbers","status":"open","priority":1,"issue_type":"task","created_at":"2025-11-27T17:16:01.004393308-06:00","updated_at":"2025-11-27T17:16:01.004393308-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-uyg","depends_on_id":"rivets-8yl","type":"blocks","created_at":"2025-11-27T17:19:32.751981569-06:00","created_by":"dwalleck"}]}
{"id":"rivets-x1e","content_hash":"1d9b77ec1210104cefd4ad1a9b8f59d67368a6379242ec33a2663fffe0dc6589","title":"Implement hash-based ID generation system","description":"Implement the adaptive hash-based ID generation system from beads, which creates collision-resistant IDs using SHA256 and base36 encoding. The system should support adaptive length (4-6 chars based on database size) and hierarchical IDs for parent-child relationships.","design":"Based on beads ids.go (288 LOC):\n\n**Algorithm**:\n1. Combine title, description, creator, timestamp, nonce\n2. SHA256 hash the content\n3. Base36 encode to desired length (4-6 chars)\n4. Format: {prefix}-{hash} (e.g., \"rivets-a3f8\")\n\n**Adaptive Length**:\n- 0-500 issues: 4 chars\n- 500-1,500: 5 chars  \n- 1,500-10,000: 6 chars\n\n**Collision Handling**:\n- Try nonces 0-99\n- If all collide, increase length\n\n**Hierarchical IDs**:\n- Parent-child: \"rivets-a3f8.1\", \"rivets-a3f8.1.2\"\n- Use child_counters table for sequence\n\n**Implementation**:\n- Use `sha2` crate for hashing\n- Custom base36 encoding function\n- Validation regex for ID format","acceptance_criteria":"- Hash ID generation function implemented\n- Adaptive length selection based on DB size\n- Collision detection and handling\n- Hierarchical ID support with dot notation\n- Base36 encoding/decoding\n- ID validation function\n- Unit tests for all scenarios (collisions, hierarchical, validation)","status":"closed","priority":1,"issue_type":"task","assignee":"claude","created_at":"2025-11-17T16:15:21.124541589-06:00","updated_at":"2025-11-17T19:58:56.62873511-06:00","closed_at":"2025-11-17T19:58:56.62873511-06:00","source_repo":"."}
{"id":"rivets-x51","content_hash":"17eaae0b44b8a2dab0260090eb3cc50e5f0193ea4beae8f80a6109a8addb150b","title":"Implement JSONL import/export system","description":"Implement the JSONL import/export system for syncing between SQLite and JSONL files, with content hash deduplication, dirty tracking, and conflict resolution.","design":"Based on beads importer (internal/importer/):\n\n**Export Flow**:\n1. Query dirty_issues table\n2. Serialize issues with embedded deps, labels, comments\n3. Compute content hash for deduplication\n4. Write to issues.jsonl (newline-delimited)\n5. Clear dirty flags\n\n**Import Flow**:\n1. Read JSONL line-by-line\n2. Parse and validate each issue\n3. Content hash comparison for conflict detection\n4. Upsert to SQLite\n5. Handle orphan dependencies (resurrect/skip/fail)\n\n**Features**:\n- Incremental dirty tracking\n- Content hash prevents duplicate writes\n- Prefix validation and migration\n- Orphan handling policies\n- External ref duplicate detection\n\n**Rust Implementation**:\n- Use serde_json for parsing\n- Streaming reader for large files\n- Batch inserts for performance","acceptance_criteria":"- Export writes issues to JSONL format\n- Import reads JSONL into SQLite\n- Content hash deduplication working\n- Dirty tracking identifies changed issues\n- Orphan handling implemented\n- Conflict resolution via content hash\n- Performance: 1000 issues in \u003c1 second\n- Integration tests for round-trip consistency","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-17T16:16:06.672016354-06:00","updated_at":"2025-11-17T17:02:47.394020398-06:00","closed_at":"2025-11-17T17:02:47.394020398-06:00","source_repo":"."}
{"id":"rivets-xy9","content_hash":"93cb37aaa5c5ae17764c12ad4b0ca4c89239639fb8406ea62efdadbd0f6ed943","title":"Implement configuration system with YAML and environment variables","description":"Implement the hierarchical configuration system that merges CLI flags, environment variables, YAML config files, and defaults with proper precedence.","design":"Based on beads internal/config/:\n\n**Precedence (high → low)**:\n1. CLI flags (--db, --json, etc.)\n2. Environment (BD_ACTOR, BD_JSON, BEADS_DIR, etc.)\n3. Config files (.beads/config.yaml, ~/.config/bd/config.yaml)\n4. Defaults\n\n**Config Schema**:\n```yaml\nissue-prefix: \"rivets\"\njson: false\nno-daemon: false\nrouting:\n  mode: auto\n  default: \".\"\nrepos:\n  primary: \".\"\n  additional: []\ndaemon:\n  auto-start: true\n  flush-debounce: \"30s\"\n```\n\n**Rust Stack**:\n- `figment` or `config` crate for merging\n- `serde` for YAML parsing\n- `dirs` for XDG config paths\n\n**Features**:\n- Walk up directory tree to find .beads/config.yaml\n- Merge multiple sources\n- Type-safe access","acceptance_criteria":"- Config merging from all sources works\n- Precedence order enforced correctly\n- YAML parsing functional\n- Environment variable support\n- CLI flag overrides working\n- Default values applied\n- Config validation with helpful errors\n- Unit tests for merge scenarios","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-17T16:16:06.99532982-06:00","updated_at":"2025-11-17T16:16:06.99532982-06:00","source_repo":"."}
{"id":"rivets-yip","content_hash":"bb0e76ef37cc3ffe620d834c6eae38b0ff111460181893356bb3a533e885672e","title":"Write comprehensive API documentation","description":"Write comprehensive rustdoc documentation for all public APIs with usage examples.\n\nDocumentation should match or exceed the quality outlined in research document.","design":"Documentation requirements:\n\n1. **Module-level docs** (lib.rs):\n   - Overview of library\n   - Quick start guide\n   - Feature comparison with other libraries\n   - Performance characteristics\n\n2. **Type documentation**:\n   - JsonlReader with examples\n   - JsonlWriter with examples\n   - JsonlQuery with examples\n   - Warning types\n   - Error types\n\n3. **Method documentation**:\n   - All public methods documented\n   - Include # Example blocks\n   - Document edge cases\n   - Document performance characteristics\n\n4. **README.md**:\n   - Installation instructions\n   - Quick start\n   - Feature list\n   - Performance benchmarks\n   - Comparison with alternatives\n\nGenerate docs with `cargo doc --no-deps --open`.","acceptance_criteria":"- All public APIs documented\n- Module-level docs comprehensive\n- README.md complete\n- All doc examples compile (tested with cargo test --doc)\n- Documentation coverage \u003e90%\n- cargo doc generates without warnings","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-27T17:18:03.937459711-06:00","updated_at":"2025-11-27T17:18:03.937459711-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-yip","depends_on_id":"rivets-kuf","type":"blocks","created_at":"2025-11-27T17:20:41.446316666-06:00","created_by":"dwalleck"}]}
{"id":"rivets-yis","content_hash":"d98e9b4a7c198849ee94471dbd936590558817d6eed94c63f6dbeae0340cc64d","title":"Implement storage backend selection via configuration","description":"Add configuration options to select storage backend (in-memory, PostgreSQL, SQLite) and pass connection parameters. Enables switching backends without code changes.","design":"Extend config system:\n\n```yaml\n# .rivets/config.yaml\nstorage:\n  backend: \"memory\"  # Options: memory, postgres, sqlite\n  \n  # For memory backend\n  data_file: \".rivets/issues.jsonl\"\n  \n  # For postgres backend (future)\n  postgres:\n    host: \"localhost\"\n    port: 5432\n    database: \"rivets\"\n    user: \"rivets\"\n    password_env: \"RIVETS_DB_PASSWORD\"\n  \n  # For sqlite backend (future)\n  sqlite:\n    path: \".rivets/rivets.db\"\n```\n\n**Config struct**:\n```rust\n#[derive(Deserialize)]\npub struct StorageConfig {\n    pub backend: StorageBackend,\n    pub data_file: Option\u003cPathBuf\u003e,\n    pub postgres: Option\u003cPostgresConfig\u003e,\n    pub sqlite: Option\u003cSqliteConfig\u003e,\n}\n\npub enum StorageBackend {\n    Memory,\n    Postgres,  // Future\n    Sqlite,    // Future\n}\n```\n\n**Factory pattern**:\n```rust\npub fn create_storage(config: \u0026StorageConfig) -\u003e Result\u003cBox\u003cdyn IssueStorage\u003e\u003e {\n    match config.backend {\n        StorageBackend::Memory =\u003e {\n            let path = config.data_file.as_ref()\n                .ok_or(Error::ConfigMissing(\"data_file\"))?;\n            \n            let storage = if path.exists() {\n                InMemoryStorage::load_from_jsonl(path)?\n            } else {\n                InMemoryStorage::new()\n            };\n            \n            Ok(Box::new(storage))\n        }\n        StorageBackend::Postgres =\u003e {\n            unimplemented!(\"PostgreSQL backend coming soon\")\n        }\n        StorageBackend::Sqlite =\u003e {\n            unimplemented!(\"SQLite backend coming soon\")\n        }\n    }\n}\n```","acceptance_criteria":"- Config supports storage backend selection\n- Factory creates correct storage implementation\n- In-memory backend configurable\n- Config validation with helpful errors\n- Environment variable support for secrets\n- Default backend is memory + JSONL\n- Integration test verifies backend switching","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-17T16:41:42.380662548-06:00","updated_at":"2025-11-17T16:41:42.380662548-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-yis","depends_on_id":"rivets-cgl","type":"blocks","created_at":"2025-11-17T16:46:30.048522789-06:00","created_by":"daemon"}]}
{"id":"rivets-zp3","content_hash":"8074c7163c84edc425be6b97ee3c06a1d39bb99c61f6b4a17fb11e7cf0052737","title":"Implement rivets-jsonl library skeleton with core modules","description":"Create the initial implementation of the rivets-jsonl library including lib.rs and stub modules for reader, writer, query, stream, and error. Set up the public API structure and basic error types.","design":"Based on rivets-kr3 design:\n- Create src/lib.rs with public API exports\n- Create src/error.rs with thiserror-based error types\n- Create src/reader.rs stub for JSONL reading operations\n- Create src/writer.rs stub for JSONL writing operations\n- Create src/query.rs stub for query/filter operations\n- Create src/stream.rs stub for streaming operations\n- Add module-level documentation\n- Create basic integration test in tests/\n- Create simple example in examples/basic.rs","acceptance_criteria":"- All module files created with proper exports from lib.rs\n- Error types defined using thiserror\n- Each module has doc comments explaining its purpose\n- Basic integration test compiles and runs\n- Example code compiles and demonstrates usage\n- `cargo test --package rivets-jsonl` passes\n- `cargo doc --package rivets-jsonl` generates without warnings","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-17T15:45:32.940600766-06:00","updated_at":"2025-11-27T17:13:36.884206539-06:00","closed_at":"2025-11-27T17:13:36.884206539-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-zp3","depends_on_id":"rivets-fk9","type":"blocks","created_at":"2025-11-17T15:45:32.942213604-06:00","created_by":"daemon"}]}
{"id":"rivets-zy0","content_hash":"487b28a274431a9b62227e7a2e164ac793cc60a0e3f5c5e0979b41f784da684f","title":"Implement streaming support for JsonlReader","description":"Implement the stream() method that returns an async Stream of deserialized records. This enables efficient processing of large JSONL files with constant memory usage.\n\nUses futures::Stream for the return type.","design":"```rust\nuse futures::stream::{Stream, StreamExt};\nuse std::pin::Pin;\n\nimpl\u003cR: AsyncRead + Unpin\u003e JsonlReader\u003cR\u003e {\n    pub fn stream\u003cT\u003e(self) -\u003e impl Stream\u003cItem = Result\u003cT\u003e\u003e\n    where\n        T: DeserializeOwned + 'static,\n    {\n        futures::stream::unfold(self, |mut reader| async move {\n            match reader.read_line().await {\n                Ok(Some(value)) =\u003e Some((Ok(value), reader)),\n                Ok(None) =\u003e None, // EOF\n                Err(e) =\u003e Some((Err(e), reader)),\n            }\n        })\n    }\n}\n```\n\nAdd futures dependency to Cargo.toml.","acceptance_criteria":"- stream() method implemented returning impl Stream\u003cItem = Result\u003cT\u003e\u003e\n- Uses futures::stream::unfold for lazy evaluation\n- Properly handles EOF (returns None to terminate stream)\n- Errors propagated through stream\n- Memory usage constant regardless of file size\n- Unit tests verify streaming behavior\n- Integration test with large file (1000+ records)","status":"open","priority":1,"issue_type":"task","created_at":"2025-11-27T17:14:45.895377787-06:00","updated_at":"2025-11-27T17:14:45.895377787-06:00","source_repo":".","dependencies":[{"issue_id":"rivets-zy0","depends_on_id":"rivets-len","type":"blocks","created_at":"2025-11-27T17:18:52.345950628-06:00","created_by":"dwalleck"}]}
